# 你的机器学习论文严谨吗？青年学者痛批学界“歪风”

<https://zhuanlan.zhihu.com/p/39578493>

作者：[Zachary C. Lipton](https://link.zhihu.com/?target=http%3A//approximatelycorrect.com/2018/07/10/troubling-trends-in-machine-learning-scholarship/)，[Jacob Steinhardt](https://link.zhihu.com/?target=http%3A//approximatelycorrect.com/2018/07/10/troubling-trends-in-machine-learning-scholarship/)

编译：Bot

编者按：这是一篇即将在ICML 2018研讨会上发表的文章，它的作者是卡内基梅隆大学助理教授Zachary C. Lipton和斯坦福大学研究生Jacob Steinhardt。虽然对于这样一个国际顶会，这两名青年学者只是籍籍无名的小辈，但近日，他们的批评却引来大量专家支持，也引发了学界的深思。去年8月，Zachary C. Lipton还就“是否该在论文引用中列出arXiv预印本”作过呼吁，他认为即便arXiv预印本质量参差不齐、没有正式发表，如果用到了其中的观点，就该保障预印本作者的权益，此事一时传为美谈。

> 因为文章过长，本文会出现意译和示例删减，还请读者包涵。

1 简介
----

机器学习（ML）研究人员的共同目标是创建和传播有关数据驱动算法的知识。在一篇论文中，他们希望自己能达成以下目标：理论概括、实验论证，或是构建准确率更高的工作架构。虽然在调研过程中选用什么知识是主观的，但一旦形成了成果，论文就应该为读者服务，解释清楚基础概念，方便读者沟通交流，换言之，它该对社区体现完整价值。

那么，什么样的论文最能服务读者？这里我们列举一些特征：这些论文应该（i）提供帮助读者理解的直觉——明确区别于有证据支持的更强有力的结论；（ii）介绍之所以排除其他假设的实证调查；（iii）明确理论分析、直觉和经验总结之间的关系；（iv）使用术语以避免概念混淆，方便读者理解。

尽管经常偏离上述思想，但近年来机器学习的新成果仍在不断涌现。本文将关注论文中常见的4种弊端，借此窥探机器学习学术圈的不良趋势：

1.  未能区分解释（explanation）和推测（speculation）；
2.  未能明确“进步”的来源，比如模型性能提高明明是因为调参，但有些作者还是会过分强调没什么效果的模型架构修改；
3.  数学性：在进行数学论证时使用模糊的、带有暗示的描述，比如混淆专业和非专业概念；
4.  滥用表述，比如用口语、艺术性语言描述成果，而不是大家认可的专业术语。

虽然这些弊端背后的原因尚未可知，但机器学习社区迅速扩张、缺乏论文审查人员、学术成就和短期成功之间的错位奖励（论文引用、关注度和创业机会）等都是可能的诱因。尽管这些弊端都有补救方法，我们还是建议大家不要这么做。

随着机器学习的影响力越来越大，研究论文的受众除了学生，还有媒体记者和政府人员，这些都是论文写作的考虑因素。通过用清晰的表达传递更准确的信息，我们可以加快研究进度、缩短新研究人员的入职时间，并在公共话语中发挥更具建设性的作用。

有缺憾的学术成果可能会误导民众，它们也会损害机器学习的知识基础，进而阻碍未来的研究。事实上，在[人工智能](https://link.zhihu.com/?target=https%3A//www.jqr.com/)的发展史中，或者更广泛地说，在科学研究中，这些问题一直是周而复始产生的。1976年，Drew McDermott曾指责人工智能社区缺乏自律，他预言“如果我们不批评自己，别人迟早会代劳”。类似的讨论贯穿整个80年代、90年代。而现在，它又出现了。对于心理学等其他领域，不严谨的实验标准曾大大削弱了这些学科的学术权威。相比之下，机器学习目前的地位是迄今为止大量严谨研究，包括理论研究和经验堆砌起来的。

2 免责声明
------

本文旨在促进讨论，ICML机器学习辩论研讨会向我们征集论文，这是我们的回应。虽然观点是我们提出的，但这里描述的问题并不是机器学习社区的通病，我们也不会讨论整体科研论文质量，更没有意愿针对具体某个个人或机构，最后得出什么批斗性结论。

这是作为内行人的关键自省，不是来自外行人的狙击。我们自己也可能陷入这些弊病，并在未来反复“病发”。虽然文中涉及一些具体示例，但我们的原则是（i）以自己作为例证；（ii）优先选择更权威、更成熟的研究人员和机构。我们为自己属于一个自由的社区感到庆幸，感谢它允许我们表达批判性观点。

3 令人不安的趋势
---------

在本节中，我们（i）描述弊端趋势；（ii）为趋势提供几个例子（包括正面例子）；（iii）解释后果。由于指出个别论文中的弱点可能是一个敏感话题，我们会尽量避免这种情况。

**3.1解释与推测**

对新领域进行探索通常需要基于直觉，但这些直觉并没有经科学验证形成正式定义。根据我们的发现，尽管这些直觉并没有经过科学审查，但一些研究人员还是会直接把它当成一个专业事实，在上面“摆事实，讲道理”，然后在推测的基础上进行解释。最后，阅读论文的人对作者的“专业素养”深信不疑，对结果信以为真，这个直觉就成了具有权威的“真理”。

例如，\[33\]这篇Google论文围绕“内部协变量转换”（internal covariate shift）提出了一个直观的理论。从摘要开始，作者就称：

> 深层网络训练时，由于模型参数在不断修改，所以各层的输入的概率分布在不断变化，这使得我们必须使用较小的学习率及较好的权重初值，导致训练很慢，同时也导致使用saturating nonlinearities 激活函数时训练很困难。这种现象加 internal covariate shift ，解决办法是：对每层的输入进行归一化。——译者注：这篇论文被视为“2015年最牛的论文”，影响力颇大

根据这些描述，这个现象和归因似乎成了个专业事实，论文也有理有据。但它的证明在哪儿？无论现实是怎样的，像这样不够清晰的关键术语解释是不足为信的。

又比如，这篇论文指出Batch Normalization可以在训练过程中通过减少隐藏激活函数的分布变化来提高模型性能，但文中丝毫没有提及对这种变化的量化方法。尽管已经有研究表明Batch Normalization的解释可能不准确\[65\]，但\[33\]给出的推测性解释已经被一些研究人员认做是事实，“众所周知，由于内部协变量转换的存在，深层神经网络网络很难优化……”\[60\]。

在\[72\]中，本文作者之一的Jacob Steinhardt也出现了同样的问题（累了，不译），但我们还是来看一个积极的例子，比如论文\[3\]。这篇文章是训练神经网络的实用指南，但作者并没有宣扬权威性，而是表示：“虽然这些建议来自……多年实验，并且在某种程度上有数学支撑，但它们应该受到挑战。它们是一个很好的起点……但没有经过正式验证，留下了许多问题亟待解决。”

**3.2未能明确“进步”的来源**

机器学习同行评审非常重视技术新颖性。为了满足评委胃口，现在许多论文都会出现复杂的模型和花哨的数学推断。虽然复杂模型本身是合理的，但它不是技术进步的唯一体现方式：巧妙的问题公式、科学实验、优化、数据预处理、大范围调参、将现有算法用于新任务……有时候，如果研究人员用许多技术实现了一个突破性成果，那他就有义务让读者明白这个成果究竟该归因于哪个必需的技术。

很多时候，作者的确提出了许多改进方法，却因为没有适当地消解研究，反而模糊了“进步”的来源。而有时，这些进步实际上只是由一项改进带来的。在这种情况下，作者看起来好像是做了很多工作，但事实上他们做的还远远不够。而这种错误印象还会误导读者，让他们以为所有改进都是必要的。

最近，Melis等人[\[54\]](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1707.05589)公布了一项成果，他们用大规模自动黑盒超参数调整重新评估一些流行RNN，发现它们的进步在于更好的超参数调整，而不是架构上的复杂创新。如果大家处于同一起跑线，那么自1997年以来几乎没有任何修改的原版LSTM还是名列前茅。比起分心去做其他研究，也许社区能从调参细节中收益更多。对于深度强化学习\[30\]和生成对抗网络\[51\]，一些类似的评估论文也引起过争议。如果想了解关于这类问题的更多内容，推荐看ICLR 2018研讨会的这篇文章[\[68\]](https://link.zhihu.com/?target=https%3A//openreview.net/forum%3Fid%3DrJWF0Fywf)。

相比之下，\[41,45,77,82\]这几篇论文对研究过程进行了很好的消解，\[10,65\]还回顾研究过程，通过分离改进找到了新发现。当然，消解对于理解方法来说既不是必要的，也不是充分的，如果有计算算力限制，实现它还可能是不切实际的。但除此之外，我们也可以通过检查模型的稳健性（鲁棒性）和定性误差分析来找出原因。

对于旨在理解的实证研究，它们甚至可以在没有新算法的情况下得到成果。比如通过探究神经网络的行为，研究人员可以区分它对对抗性扰动的敏感性\[74\]；通过仔细研究，他们可以发现数据集对更强大基线模型的限制；论文\[11\]研究设计用于阅读理解新闻段落的任务，发现其中有73%的问题可以从同一个句子中找到答案，而只有2%的问题需要查看多个句子。此外，本着同样的精神，比起复杂的神经架构，更简单的神经网络和线性分类器往往表现更好。

**3.3数学性**

博士前期写论文时，我们（ZL）曾收到一位经验丰富的博士后的反馈：你们的论文需要更多方程。他没有评判论文成果，只是建议论文看起来应该更清晰一些。即便论文内容晦涩难懂，如果里面包含更多计算方程，评审员也会认为它有过人的专业深度。

数学是科学交流的重要工具，如果使用方法正确，它传递的信息是高度精确和清晰的。然而，并不是所有想法和主张都适合用数学描述，自然语言同样是一种不可或缺的交流工具，它在表述直觉和经验主张时尤为突出。

当我们把数学和自然语言结合在一起，却没有明确它们的关系时，无论是散文还是理论，我们都表述不好：理论中的问题可能会用模糊的定义来概述，散文中的情感抒发却可以被数理推断来“论证”。数学是正式和非正式表述的结合体，就像经济学家Paul Romer所说的：就像数学理论一样，数学是语言和符号的混合，但它没有做紧密联系，而是在自然语言和形式语言之间留下一个充足的平滑空间。

伴随数学性产生的弊端主要表现在以下几方面：

首先，一些论文会滥用数学来体现文章的深度——强行有深度。假定理其中最常见的形式，通过把定理插进论文中，论文有了实证结果，看起来也更具权威性——即便这个定理和论文完全没关系。我们（JS）在\[70\]里就犯了这个错，论文中对“staged strong Doeblin chains”的讨论和提出的算法几乎没什么关系，但读者可能会觉得很有深度。

Adam这篇论文[\[35\]](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1412.6980)很好，但也证明这个问题无处不在。论文中介绍了凸优化情况下的收敛定理，然而这不是一篇凸优化论文，这是不必要的。后来，\[63\]也证实那是错误的。

其次，一些论文的数学表述既不正式，也不非正式。举个例子，\[18\]这篇论文认为神经网络的优化困难不是源于局部最小值，而是鞍点。作为证据，他们引用了一篇关于高斯随机场的统计物理论文\[9\]，指出高维“高斯随机场的所有局部最小值都可能有一个非常接近全局最小值的误差”。这看起来是一个很正式的主张，但缺乏一个明确的定理，所以它的结果很难验证。如果研究人员能给出一个正式的声明，这里的疑问就可以被解决了。在\[18\]中，研究人员发现局部最小值比鞍点具有更低的损失，也给出了清晰的说明和实验，内容也更有趣。

最后，一些论文引用的理论太宽泛了，是否能在这个场景下使用这个定理还存疑。比如一些人喜欢用“天底下没有免费的午餐”来类比使用没有保证的启发式方法，但这句话的本意并没有说我们不能学习啊。

虽然避免使用方程是对数学性问题的最佳补救方法，但一些论文也以身示范，证明数学不是“洪水猛兽”。比如最近发表的论文\[8\]以扎实的方式涵盖大量数学基础，而且这些数据计算还和应用问题有明确联系。我们在此强烈推荐这篇论文，刚入行的新手也可以参考着研究他们的方向。

**3.4滥用表述**

我们找出了机器学习论文中滥用表述的三种常见形式：暗示性定义、滥用已有术语、滥用行李箱词。

*   3.4.1暗示性定义

暗示性定义指的是新造一个专业术语，它具有暗示性的口语含义，只看字面意思就能理解。这些词经常出现在拟人化任务（阅读理解\[31\]、音乐理解\[59\]）和技巧型任务（好奇\[66\]、害怕\[48\]）中。许多论文会以人类认知的方式来命名模型的组成部分，比如“思想载体”\[36\]和“意识先验”\[4\]。我们不是说这些词一定不能用，如果合格，它们和机器学习的关系可能会成为创建表述的有效灵感来源。然而，当一个暗示性定义被当成专业术语时，今后的论文就没有选择余地了，研究人员只能用这个术语，否则读者会感到困惑。

另一方面，用“人类”表现来描述机器学习成果可能会产生对当前技术水平的认知错误。以\[21\]中的“皮肤病专家级皮肤癌分类”为例，研究人员通过用分类器和皮肤科专家进行比较，掩盖了两者执行的任务有本质区别的事实。真正的皮肤科专家会遇到各种情况，尽管有不可预测的变化，但他们必须给出诊断意见，而分类器只是在测试数据上实现了低误差。同样的，\[29\]里分类器也称自己在ImageNet分类任务上比人类更具优势。试想一下，在那么多“口无遮拦”\[21,57,75\]的论文中，即便我们有一篇表述严谨的，它能让公众话语重回正轨吗？

虽然深度学习论文不是唯一的“始作俑者”，但这个领域滥用表述的现象确实影响到了其他机器学习子域的研究。比如\[49\]研究的是算法“公平性”的问题，它很好地展示了研究人员是怎么用法律术语搞机器学习研究的，里面最突出的例子是他们把一个表达统计平等概念的简单方程命名为“不同的影响”。由此产生的问题就是大家开始参考着用“公平”“机会”和“歧视”表示简单预测模型的统计数据，然后民众和政府官员就会误以为把道德需求纳入机器学习是一件很艰难的事。

*   3.4.2滥用已有术语

第二种滥用方式包括使用已有的专用术语，但是会以不精确，甚至互相矛盾的方式来使用它。比如deconvolution（转置卷积、反卷积、逆卷积、去卷积╮(╯_╰)╭），它描述的是卷积的逆运算，但在深度学习论文中，尤其是自动编码器和生成对抗网络论文中，这个词却被等同于transpose convolutions（转置卷积，也称up-convolutions上卷积）。当\[79\]第一次在深度学习论文中提到这个词时，它的定义还是准确的，但\[78,50\]一引用概括，它就成了任何使用上卷积的神经架构。这种术语的滥用会造成持久的混乱，如果现在有一篇新机器学习论文，里面出现了deconvolution，它的意思可能是（i）原始含义，（ii）上卷积，或（iii）试图解决这种混淆\[28\]。

作为另一个例子，我们来看生成模型（generative model）和判别模型（discriminative model）。从一般定义上来说，如果输入的分布是p(x)或是联合分布p(x,y)，它就是个生成模型；相反地，判别模型处理的是条件概率分布P(y|x)。然而，在最近的论文中，“生成模型”成了产生逼真结构化数据的模型的统称。从表面上看，这和定义似乎没有冲突，但它掩盖了几个缺点——例如，GAN和VAE无法执行条件推理（x1和x2是两个不同的输入特征，它们无法从p(x2|x1)中采样）。在这个曲解的基础上，一些人也开始把判别模型形容成负责生成结构化输出的生成模型\[76\]。我们（ZL）在\[47\]中也犯了这个错误。

我们继续看之前提到的Batch Normalization，\[33\]把协变量转换（covariate shift）描述为模型输入分布的变化，事实上，这个词指的是特定类型的转换——尽管输入分布p(x)可能会发生变化，但p(y|x)不会变\[27\]。此外，由于\[33\]的误用，现在谷歌学术已经把Batch Normalization列为“协变量转换”的第一个参考项。

像这样滥用已有术语的后果之一是我们可以通过“偷换概念”来定义一些未解决任务，然后方便自己引用以往成果，从而包装没什么实质进展的“进步”。它通常会和暗示性定义相结合。语言理解和阅读理解，这些都曾是AI的巨大挑战，现在却成了对特定数据集做出预测\[31\]。

*   3.4.3行李箱词

最后，我们来看ML论文中常见的过度使用行李箱词（Suitcase Words）。这是Minsky在2007年出版的《The Emotion Machine》\[56\]中创造的新词，指的是汇集了各种意义的一类词汇，比如意识、思维、注意力、情感和感觉，它们的生成机制和来源也许不同，但我们可以把它们统称为“心理过程”。机器学习中有很多类似的词汇，如\[46\]就指出“可解释性”这个词并没有一个普遍认可的含义，它常出现在不同方法、不同需求的论文中。因此，虽然论文表述看起来差不多，但它们的也许表达不同的概念。

再如generalization，这个词可以概括一项特定技术（概括训练到测试），也可以表示互相接近的两个概念之间的转移（从一个群体推广到另一个群体），甚至还能衍生到外部（从实验环境推广到现实环境）。如果把这些概念混为一谈，我们会高估当前技术的水平。

当暗示性定义和滥用已有术语相结合时，新的行李箱词往往随之而生。在涉及“公平性”的论文中，法律、哲学、统计语言学的术语经常被滥用，这些词随后就会被一个叫“偏见”的词笼统概括\[17\]。

如果是演讲或是谈理想，行李箱词确实可以起到有效的作用，因为它反映了将各种含义统一起来的总体概念，比如人工智能就是一个理想名词。另一方面，在技术论证过程中过多地使用行李箱词可能会导致混淆，例如\[6\]这本书中用术语和优化能力写了一些等式，非常不严谨地把它们假设为同一类东西。

4 趋势背后的原因
---------

上述问题是否是ML学术圈的一种趋势？如果是，那么根本原因是什么？我们进行了一些推测，最后得出了几个可能的因果因素：

**4.1面对进步开始骄傲自满**

ML的快速发展会让研究人员产生一种错觉，即强有力的结果可以掩盖论证过程的弱小。所以他们开始为了支持结论插入并没有关系的东西，开始以结果为目标设置实验，开始使用夸张的表述，或者不再尽力避免不严谨的数学推断。

与此同时，在大量同质化论文面前，论文审稿人别无选择，只能接受有强大定量结果的论文。实际上，即便论文这次被拒了，他们也不能保证下次能注意到其中的缺陷，所以接受有缺陷的论文反而成了一件好事。

**4.2成长的痛苦**

自2012年来，深度学习大获成功，人们对学界的追捧也日益热烈，因此ML社区迅速扩张。虽然我们认为社区扩大是好事，但它也会产生副作用。

为了保护初级作者，这篇文章内我们以引用自己的论文和引用大机构的论文为主，但我们不说不代表不存在，上述问题在他们的论文中更常见。一些初级作者会因为不清楚术语的定义而擅自把它重新定义一遍，当然，经验丰富的研究人员身上也有这种毛病。

对于论文审阅，也许提高论文-审稿人比例可以改善这种情况，但问题还是存在的。经验不足的审稿人更关注论文的新颖性，他们往往会被虚假定理蒙蔽双眼；经验丰富的审稿人往往承担更多工作，他们会相对保守，更喜欢有很多数学公式的论文，会忽视创新型研究；而剩下的大批过度工作的审稿人连审稿的时间都不够，他们注意不到论文的诸多问题。

**4.3激励措施的错位**

审稿人不是唯一一群给论文作者提供不良激励的人，随着ML研究越来越受媒体关注，ML创业公司变得司空见惯，在某种程度上，新闻（报道什么）和投资者（投资什么）才是激励的主体。媒体引导着ML研究趋势，而ML算法中的拟人化表述则为话题的流行提供源源不断的素材。以\[55\]为例，它把自动编码器描述为“模拟大脑”，这种暗示放在新闻头条上是耸人听闻的；又如\[52\]，它把用深度学习给图像生成描述写成“模仿人类理解水平”。

投资者也对人工智能研究表现出了强烈兴趣，有时他们甚至会因为一篇论文就给创业公司提供资金。我们（ZL）也曾和投资者有过合作，媒体报道了哪家初创公司的成果，他们就投资哪家，这种动态的财务激励和媒体关注是捆绑的。我们注意到，最近投资界对聊天[机器人](https://link.zhihu.com/?target=http%3A//www.jqr.com/)的兴趣浓厚，而这是和媒体大肆报道对话系统、强化学习同时出现的。

5 建议
----

面对这些趋势，我们又该如何应对？我们要怎么做才能让社区提高实验实践、阐述和理论的水平？我们要怎么做才能更容易地提炼社区的知识，并消除广大公众对研究的误解？（不干了，开偷懒大招了）

**5.1对作者的建议**

*   多问“为什么”和“怎么做”，而不仅仅是“效果有多好”。在实证论文中多用错误分析、消解研究和稳健性检查（例如仔细调参、选择理想的数据集），多看多读多引用。
*   不要强行为了用特定算法而找出它对研究课题的进步贡献。即便没有新算法，你也可以在课题上产生一些新见解，比如通过随机梯度下降训练的神经网络可以拟合随机分配的标签。
*   在写论文时，你要问自己：我提出的这个系统，我自己是不是很认可，会不会在实践中使用？这可以模拟审稿人看到这篇文章时的想法，还能检测这个系统是不是真的符合你心目中的智能模型。
*   明确哪些问题是开放的，那些问题已经被解决了，要清晰地了解研究现状。

**5.2对出版商和审稿人的建议**

*   问自己：如果这个作者的成果做得更差一点，我还会接受这篇论文吗？比如有两篇文章得出的结论差不多，但第一篇用一个简单的想法就实现了改进，而且给出了两个否定结果，第二篇结合三种想法实现了相同的改进（没有消解），那就应该选第一篇。
*   进行回顾性调查，要求删去夸张的主张和无关材料，把拟人表述改成明确的术语和符号。
*   呼吁批判性论文，挑战传统思维。
*   同行评审体系有待进一步讨论：公开评审还是匿名评审？审稿人如何代表大多数研究人员的价值观？这些改进会对上述弊端的改善带来什么后果？

后面还有一些零散内容，此处不再翻译（字数超了，放不下了）。综合全文，这些问题确实是许多论文中常见的问题，小编在啃论文时，也会被滥用的术语、行李箱词折磨地抓狂，最后可能误读，继而误导更多读者。而误读的又怎么会只有小编一人？

如果大家耐心读到了这里，希望这篇文章能让我们吸取教训，无论是初学者、研究人员还是新闻媒体，我们都希望见证机器学习领域的健康发展，而不想让夸夸其谈毁掉前辈用严谨的治学态度留给我们的基业。

参考文献
----

![](https://img.hacpai.com/e/a587830d89db49e28405f15e81dc2a7d.jpeg)

![](https://img.hacpai.com/e/e8451fef658640f1beda44bc1a32cadc.jpeg)

![](https://img.hacpai.com/e/f5252abce6af40ddbf2ac685aea01e21.jpeg)

![](https://img.hacpai.com/e/7acaf81fdb58488b963b9db7ac2280d1.jpeg)

![](https://img.hacpai.com/e/da213c86d0df47c1ad80971e4b6641c6.jpeg)

![](https://img.hacpai.com/e/7f56c37970b84f0a91d6b092e53e47a6.jpeg)

![](https://img.hacpai.com/e/be030cb6d0bb4b348032c44daedcd09e.jpeg)