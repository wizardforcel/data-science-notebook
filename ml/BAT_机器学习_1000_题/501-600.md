## 501题

下列属于无监督学习的是

A、k-means

B、SVM

C、最大熵

D、CRF



正确答案是：A

解析：

正确答案：A A是聚类，属于无监督学习。BC是分类，属于监督学习。至于D是序列化标注，也是有监督学习。

## 502题

下列哪个不属于CRF模型对于HMM和MEMM模型的优势（ ）

A、特征灵活

B、速度快

C、可容纳较多上下文信息

D、全局最优



正确答案是： B

解析：

CRF 的优点：特征灵活，可以容纳较多的上下文信息，能够做到全局最优CRF 的缺点：速度慢CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息。特征设计灵活（与ME一样） ————与HMM比较 

同时，由于CRF计算全局最优输出节点的条件概率，它还克服了最大熵马尔可夫模型标记偏置（Label-bias）的缺点。 ­­————与MEMM比较 

CRF是在给定需要标记的观察序列的条件下，使用维特比算法，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。————与ME比较

## 503题

以下哪个是常见的时间序列算法模型

A、RSI

B、MACD

C、ARMA

D、KDJ



正确答案是：C

解析：

自回归滑动平均模型(ARMA) 其建模思想可概括为：逐渐增加模型的阶数，拟合较高阶模型，直到再增加模型的阶数而剩余残差方差不再显著减小为止。 

其他三项都不是一个层次的。

A.相对强弱指数 (RSI, Relative Strength Index) 是通过比较一段时期内的平均收盘涨数和平均收盘跌数来分析市场买沽盘的意向和实力 , 从而作出未来市场的走势 . 

B.移动平均聚散指标 (MACD, Moving Average Convergence Divergence), 是根据均线的构造原理 , 对股票价格的收盘价进行平滑处理 , 求出算术平均值以后再进行计算 , 是一种趋向类指标 . 

D. 随机指标 (KDJ) 一般是根据统计学的原理 , 通过一个特定的周期 ( 常为 9 日 ,9 周等 ) 内出现过的最高价 , 最低价及最后一个计算周期的收盘价及这三者之间的比例关系 , 来计算最后一个计算周期的未成熟随机值 RSV, 然后根据平滑移动平均线的方法来计算 K 值 , D 值与 J 值 , 并绘成曲线图来研判股票走势 .

## 504题

下列不是SVM核函数的是

A、多项式核函数

B、logistic核函数

C、径向基核函数

D、Sigmoid核函数



正确答案是： B

解析：

@刘炫320，本题题目及解析来源：http://blog.csdn.net/column/details/16442.html 

SVM核函数包括线性核函数、多项式核函数、径向基核函数、高斯核函数、幂指数核函数、拉普拉斯核函数、ANOVA核函数、二次有理核函数、多元二次核函数、逆多元二次核函数以及Sigmoid核函数。

核函数的定义并不困难，根据泛函的有关理论，只要一种函数 K ( x i , x j ) 满足Mercer条件，它就对应某一变换空间的内积．对于判断哪些函数是核函数到目前为止也取得了重要的突破，得到Mercer定理和以下常用的核函数类型：

(1)线性核函数 K ( x , x i ) = x ⋅ x i 

(2)多项式核 K ( x , x i ) = ( ( x ⋅ x i ) + 1 ) d 

(3)径向基核（RBF） K ( x , x i ) = exp ( − ∥ x − x i ∥ 2 σ 2 ) Gauss径向基函数则是局部性强的核函数，其外推能力随着参数 σ 的增大而减弱。多项式形式的核函数具有良好的全局性质。局部性较差。 

(4)傅里叶核 K ( x , x i ) = 1 − q 2 2 ( 1 − 2 q cos ( x − x i ) + q 2 ) 

(5)样条核 K ( x , x i ) = B 2 n + 1 ( x − x i ) 

(6)Sigmoid核函数 K ( x , x i ) = tanh ( κ ( x , x i ) − δ )

采用Sigmoid函数作为核函数时，支持向量机实现的就是一种多层感知器神经网络，应用SVM方法，隐含层节点数目(它确定神经网络的结构)、隐含层节点对输入节点的权值都是在设计(训练)的过程中自动确定的。而且支持向量机的理论基础决定了它最终求得的是全局最优值而不是局部最小值，也保证了它对于未知样本的良好泛化能力而不会出现过学习现象。

核函数的选择在选取核函数解决实际问题时，通常采用的方法有： 

一是利用专家的先验知识预先选定核函数；

二是采用Cross-Validation方法，即在进行核函数选取时，分别试用不同的核函数，归纳误差最小的核函数就是最好的核函数．如针对傅立叶核、RBF核，结合信号处理问题中的函数回归问题，通过仿真实验，对比分析了在相同数据条件下，采用傅立叶核的SVM要比采用RBF核的SVM误差小很多． 

三是采用由Smits等人提出的混合核函数方法，该方法较之前两者是目前选取核函数的主流方法，也是关于如何构造核函数的又一开创性的工作．将不同的核函数结合起来后会有更好的特性，这是混合核函数方法的基本思想．

## 505题

解决隐马模型中预测问题的算法是

A、前向算法

B、后向算法

C、Baum-Welch算法

D、维特比算法



正确答案是：D

解析：

@刘炫320，本题题目及解析来源：http://blog.csdn.net/column/details/16442.html 

A、B：前向、后向算法解决的是一个评估问题，即给定一个模型，求某特定观测序列的概率，用于评估该序列最匹配的模型。 

C：Baum-Welch算法解决的是一个模型训练问题，即参数估计，是一种无监督的训练方法，主要通过EM迭代实现； 

D：维特比算法解决的是给定 一个模型和某个特定的输出序列，求最可能产生这个输出的状态序列。如通过海藻变化（输出序列）来观测天气（状态序列），是预测问题，通信中的解码问题。

## 506题

一般，k-NN最近邻方法在（）的情况下效果较好

A、样本较多但典型性不好

B、样本较少但典型性好

C、样本呈团状分布

D、样本呈链状分布



正确答案是： B

解析：

K近邻算法主要依靠的是周围的点，因此如果样本过多，那肯定是区分不出来的。因此应当选择B样本呈团状颇有迷惑性，这里应该指的是整个样本都是呈团状分布，这样kNN就发挥不出其求近邻的优势了，整体样本应该具有典型性好，样本较少，比较适宜。

## 507题

在一个n维的空间中， 最好的检测outlier(离群点)的方法是（）

A、作正态分布概率图

B、作盒形图

C、马氏距离

D、作散点图



正确答案是：C

解析：

马氏距离是基于卡方分布的，度量多元outlier离群点的统计方法。有M个样本向量X1~Xm，协方差矩阵记为S，均值记为向量μ，则其中样本向量X到u的马氏距离表示为：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS9qL98XVrOmPaicj67qNP1BMR6M3WrgCAlPYw7oLmuJXetNuVYbCAKiaBwbTu6jYzKlQeACkFPy1Czw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

（协方差矩阵中每个元素是各个矢量元素之间的协方差Cov(X,Y)，Cov(X,Y) = E{ [X-E(X)] [Y-E(Y)]}，其中E为数学期望）而其中向量Xi与Xj之间的马氏距离定义为：


![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS9qL98XVrOmPaicj67qNP1BMvGjicRbjaM0ib8hibicwt4ia75qpnkagQiatj123ggNfCpXeCB2A3LxWicgEQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

若协方差矩阵是单位矩阵（各个样本向量之间独立同分布）,则公式就成了：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS9qL98XVrOmPaicj67qNP1BMO90EqeicURmFTDPzAHMLF3gBPhUQ8U2ib01Z7N0YpbSia1VvPJKbpiaLPA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

也就是欧氏距离了。　　

若协方差矩阵是对角矩阵，公式变成了标准化欧氏距离。 

(2)马氏距离的优缺点：量纲无关，排除变量之间的相关性的干扰。

## 508题

对数几率回归（logistics regression）和一般回归分析有什么区别？

A、对数几率回归是设计用来预测事件可能性的

B、对数几率回归可以用来度量模型拟合程度

C、对数几率回归可以用来估计回归系数

D、以上所有



正确答案是：D

解析：

A: 对数几率回归其实是设计用来解决分类问题的 

B: 对数几率回归可以用来检验模型对数据的拟合度 

C: 虽然对数几率回归是用来解决分类问题的，但是模型建立好后，就可以根据独立的特征，估计相关的回归系数。就我认为，这只是估计回归系数，不能直接用来做回归模型。

## 509题

bootstrap数据是什么意思？（提示：考“bootstrap”和“boosting”区别）

A、有放回地从总共M个特征中抽样m个特征

B、无放回地从总共M个特征中抽样m个特征

C、有放回地从总共N个样本中抽样n个样本

D、无放回地从总共N个样本中抽样n个样本



正确答案是：C

解析：

boostrap是提鞋自举的意思(武侠小说作者所说的左脚踩右脚腾空而起). 它的过程是对样本(而不是特征)进行有放回的抽样, 抽样次数等同于样本总数. 这个随机抽样过程决定了最终抽样出来的样本, 去除重复之后, 占据原有样本的1/e比例.

## 510题

“过拟合”只在监督学习中出现，在非监督学习中，没有“过拟合”，这是（）

A、对的

B、错的



正确答案是： B

解析：

我们可以评估无监督学习方法通过无监督学习的指标，如：我们可以评估聚类模型通过调整兰德系数（adjusted rand score）https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index

## 511题

对于k折交叉验证, 以下对k的说法正确的是（）

A、k越大, 不一定越好, 选择大的k会加大评估时间

B、选择更大的k, 就会有更小的bias (因为训练集更加接近总数据集)

C、在选择k时, 要最小化数据集之间的方差

D、以上所有



正确答案是：D

解析：

k越大, bias越小, 训练时间越长. 在训练时, 也要考虑数据集间方差差别不大的原则. 比如, 对于二类分类问题, 使用2-折交叉验证, 如果测试集里的数据都是A类的, 而训练集中数据都是B类的, 显然, 测试效果会很差. 

如果不明白bias和variance的概念, 务必参考下面链接:Gentle Introduction to the Bias-Variance Trade-Off in Machine Learninghttp://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/Understanding the Bias-Variance Tradeoffhttp://scott.fortmann-roe.com/docs/BiasVariance.html

## 512题

回归模型中存在多重共线性, 你如何解决这个问题？

1 去除这两个共线性变量 

2 我们可以先去除一个共线性变量 

3 计算VIF(方差膨胀因子), 采取相应措施 

4 为了避免损失信息, 我们可以使用一些正则化方法, 比如, 岭回归和lasso回归

A、1

B、2

C、2和3

D、2, 3和4



正确答案是：D

解析：

解决多重公线性, 可以使用相关矩阵去去除相关性高于75%的变量 (有主观成分). 也可以VIF, 如果VIF值<=4说明相关性不是很高, VIF值>=10说明相关性较高. 

我们也可以用 岭回归和lasso回归的带有惩罚正则项的方法. 我们也可以在一些变量上加随机噪声, 使得变量之间变得不同, 但是这个方法要小心使用, 可能会影响预测效果。

## 513题

模型的高bias是什么意思, 我们如何降低它 ？

A、在特征空间中减少特征

B、在特征空间中增加特征

C、增加数据点

D、B和C

E、以上所有



正确答案是： B

解析：

bias太高说明模型太简单了, 数据维数不够, 无法准确预测数据, 所以, 升维吧 !

## 514题

训练决策树模型, 属性节点的分裂, 具有最大信息增益的图是下图的哪一个（）

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8cvJ0RNaIqLEN4T14H2G1qGBemTtTWR6Ma5gkiaYxxE9DuFtlw6hdPUN9esKdM5UiaHATZGXiatsAGw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

A、Outlook

B、Humidity

C、Windy

D、Temperature



正确答案是：A

解析：

A信息增益, 增加平均子集纯度, 详细研究, 请戳看相关论文:A Complete Tutorial on Tree Based Modeling from Scratch (in R & Python)Lecture 4 Decision Trees (2): Entropy, Information Gain, Gain Ratio

## 515题

对于信息增益, 决策树分裂节点, 下面说法正确的是（）

1 纯度高的节点需要更多的信息去区分 

2 信息增益可以用”1比特-熵”获得 

3 如果选择一个属性具有许多归类值, 那么这个信息增益是有偏差的

A、1

B、2

C、2和3

D、所有以上



正确答案是：C

解析：

C详细研究, 请看相关论文:A Complete Tutorial on Tree Based Modeling from Scratch (in R & Python)Lecture 4 Decision Trees (2): Entropy, Information Gain, Gain Ratio

## 516题

下图是同一个SVM模型, 但是使用了不同的径向基核函数的gamma参数, 依次是g1, g2, g3 , 下面大小比较正确的是

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8cvJ0RNaIqLEN4T14H2G1qt49Wda1L1H4GU7bOx933np0LNlxjaPhQibCxvCHsVSWgZwb0UAypvibA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

A、g1 > g2 > g3

B、g1 = g2 = g3

C、g1 < g2 < g3

D、g1 >= g2 >= g3E. g1 <= g2 <= g3



正确答案是：C

解析：

所谓径向基函数 (Radial Basis Function 简称 RBF), 就是某种沿径向对称的标量函数。 通常定义为空间中任一点x到某一中心点xc之间欧氏距离的单调函数 , 可记作 k(||x-xc||), 其作用往往是局部的 , 即当x远离xc时函数取值很小。

最常用的径向基函数是高斯核函数 ,形式为 k(||x-xc||)=exp{- ||x-xc||^2/(2*σ^2) } 其中xc为核函数中心,σ为函数的宽度参数 , 控制了函数的径向作用范围。由radial basis: exp(-gamma*|u-v|^2)可知, gamma越小, 模型越简单, 平滑度越好, 分类边界越不容易过拟合, 所以选C。

## 517题

假设我们要解决一个二类分类问题, 我们已经建立好了模型, 输出是0或1, 初始时设阈值为0.5, 超过0.5概率估计, 就判别为1, 否则就判别为0 ; 如果我们现在用另一个大于0.5的阈值,  那么现在关于模型说法, 正确的是 : 

1 模型分类的召回率会降低或不变 

2 模型分类的召回率会升高 

3 模型分类准确率会升高或不变 

4 模型分类准确率会降低

A、1

B、2

C、1和3

D、2和4

E、以上都不是



正确答案是：A

解析：

精确率, 准确率和召回率是广泛用于信息检索和统计学分类领域的度量值，用来评价结果的质量。下图可以帮助理解和记忆它们之间的关系, 其中精确率(precision)和准确率(accuracy)都是关于预测效果的描述. 召回率是关于预测样本的描述。

精确率表示的是预测为正的样本中有多少是真正的正样本。那么预测为正就有两种可能了，一种就是把正类预测为正类(TP)，另一种就是把负类预测为正类(FP), 也就是P = TP / (TP + FP)。 

准确率表示的是预测的正负样本有多少是真实的正和负, 预测正确的数量占全部预测数量的比例, 也就是A = (TP + TN) / (TP + FP + TN + FN) = (TP + TN) / 全部样本。

召回率表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN), 也就是R = TP / (TP + FN)。 

精确率和召回率二者计算方法其实就是分母不同，一个分母是预测为正的样本数，另一个是原来样本中所有的正样本数。 

提高分界阈值大于0.5, 则预测为正的样本数要降低, 相当于把图中圆圈变小, 按下图则可计算

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8cvJ0RNaIqLEN4T14H2G1qJEjZ5t1QJ8n4LSibSzBSmUX5NiavicT0T2a3w9bKMC515h8YkGYYgmneA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

召回率的分子变小分母不变, 所以召回率会变小或不变; 

精确率的分子分母同步变化, 所以精确率的变化不能确定;

准确率的分子为圆内绿色加圆外右侧矩形面积所围样本, 两者之和变化不能确定; 分母为矩形所含全部样本不变化, 所以准确率的变化不能确定; 

综上, 所以选A。


## 518题

“点击率问题”是这样一个预测问题, 99%的人是不会点击的, 而1%的人是会点击进去的, 所以这是一个非常不平衡的数据集. 假设, 现在我们已经建了一个模型来分类, 而且有了99%的预测准确率, 我们可以下的结论是

A、模型预测准确率已经很高了, 我们不需要做什么了

B、模型预测准确率不高, 我们需要做点什么改进模型

C、无法下结论

D、以上都不对



正确答案是：C

解析：

如寒老师所说，类别不均衡的情况下，不要用准确率做分类评估指标，因为全判断为不会点，准确率也是99%，但是这个分类器一点用都没有。 

详细可以参考这篇文章：https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/

## 519题

使用k=1的knn算法, 下图二类分类问题, “+” 和 “o” 分别代表两个类, 那么, 用仅拿出一个测试样本的交叉验证方法, 交叉验证的错误率是多少：

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS8cvJ0RNaIqLEN4T14H2G1qibZZeUwXBHSfV6HsglRhCGhuW7zVOuEXAiaicib1jaqCF825YD1m1RvVqg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

A、0%

B、100%

C、0%到100

D、以上都不是



正确答案是： B

解析：

knn算法就是, 在样本周围看k个样本, 其中大多数样本的分类是A类, 我们就把这个样本分成A类. 显然, k=1 的knn在上图不是一个好选择, 分类的错误率始终是100%。

## 520题

我们想在大数据集上训练决策树, 为了使用较少时间, 我们可以

A、增加树的深度

B、增加学习率 (learning rate)

C、减少树的深度

D、减少树的数量



正确答案是：C

解析：

增加树的深度, 会导致所有节点不断分裂, 直到叶子节点是纯的为止. 所以, 增加深度, 会延长训练时间.决策树没有学习率参数可以调. (不像集成学习和其它有步长的学习方法)决策树只有一棵树, 不是随机森林。

## 521题

假如我们使用非线性可分的SVM目标函数作为最优化对象, 我们怎么保证模型线性可分？

A、设C=1

B、设C=0

C、设C=无穷大

D、以上都不对



正确答案是：C

解析：

答案: CC无穷大保证了所有的线性不可分都是可以忍受的

## 522题

以下哪些算法, 可以用神经网络去构造:

1) KNN

2) 线性回归

3) 对数几率回归

A、1和 2

B、2 和 3

C、1, 2 和 3

D、以上都不是



正确答案是： B

解析：

答案: B 

1)KNN算法不需要训练参数, 而所有神经网络都需要训练参数, 因此神经网络帮不上忙

2) 最简单的神经网络, 感知器, 其实就是线性回归的训练

3) 我们可以用一层的神经网络构造对数几率回归

## 523题

请选择下面可以应用隐马尔科夫(HMM)模型的选项

A、基因序列数据集

B、电影浏览数据集

C、股票市场数据集

D、所有以上



正确答案是：D

解析：

答案: D只要是和时间序列问题有关的 , 都可以试试HMM.

## 524题

我们建立一个5000个特征, 100万数据的机器学习模型. 我们怎么有效地应对这样的大数据训练 :

A、我们随机抽取一些样本, 在这些少量样本之上训练

B、我们可以试用在线机器学习算法

C、我们应用PCA算法降维, 减少特征数

D、B 和 C

E、A 和 B

F、以上所有



正确答案是：F

解析：

样本数过多, 或者特征数过多, 而不能单机完成训练, 可以用小批量样本训练, 或者在线累计式训练, 或者主成分PCA降维方式减少特征数量再进行训练.

## 525题

我们想要减少数据集中的特征数, 即降维. 选择以下适合的方案 :

1) 使用前向特征选择方法

2) 使用后向特征排除方法

3) 我们先把所有特征都使用, 去训练一个模型, 得到测试集上的表现. 然后我们去掉一个特征, 再去训练, 用交叉验证看看测试集上的表现. 如果表现比原来还要好, 我们可以去除这个特征. 

4) 查看相关性表, 去除相关性最高的一些特征

A、1 和 2

B、2, 3和4

C、1, 2和4

D、All



正确答案是：D

解析：

答案: D 

1)前向特征选择方法和后向特征排除方法是我们特征选择的常用方法

2)如果前向特征选择方法和后向特征排除方法在大数据上不适用, 可以用这里第三种方法. 

3)用相关性的度量去删除多余特征, 也是一个好方法 

所以D是正确的

## 526题

对于随机森林和GradientBoosting Trees, 下面说法正确的是:

1 在随机森林的单个树中, 树和树之间是有依赖的, 而GradientBoosting Trees中的单个树之间是没有依赖的 

2 这两个模型都使用随机特征子集, 来生成许多单个的树

3 我们可以并行地生成GradientBoosting Trees单个树, 因为它们之间是没有依赖的 

4 GradientBoosting Trees训练模型的表现总是比随机森林好

A、2

B、1 and 2

C、1, 3 and 4

D、2 and 4



正确答案是：A

解析：

答案: A 

1 随机森林是基于bagging的, 在随机森林的单个树中, 树和树之间是没有依赖的。 

2 Gradient Boosting trees是基于boosting的，且GradientBoosting Trees中的单个树之间是有依赖关系。 

3 这两个模型都使用随机特征子集, 来生成许多单个的树。

所以题干中只有第二点是正确的，选A。 


更多详情请参见《通俗理解kaggle比赛大杀器xgboost》：https://blog.csdn.net/v_JULY_v/article/details/81410574，循序渐进，先后理解：决策树、CBDT、xgboost。

## 527题

对于PCA(主成分分析)转化过的特征 ,  朴素贝叶斯的”不依赖假设”总是成立, 因为所有主要成分是正交的, 这个说法是 :

A、正确的

B、错误的



正确答案是： B

解析：

答案: B. 

这个说法是错误的, 首先, “不依赖”和”不相关”是两回事, 其次, 转化过的特征, 也可能是相关的

## 528题

对于PCA说法正确的是 :

1) 我们必须在使用PCA前规范化数据

2) 我们应该选择使得模型有最大variance的主成分

3) 我们应该选择使得模型有最小variance的主成分

4) 我们可以使用PCA在低维度上做数据可视化

A、1, 2 and 4

B、2 and 4

C、3 and 4

D、1 and 3

E、1, 3 and 4



正确答案是：A

解析：

答案: A 

1）PCA对数据尺度很敏感, 打个比方, 如果单位是从km变为cm, 这样的数据尺度对PCA最后的结果可能很有影响(从不怎么重要的成分变为很重要的成分). 

2）我们总是应该选择使得模型有最大variance的主成分 

3）有时在低维度上左图是需要PCA的降维帮助的

## 529题

对于下图, 最好的主成分选择是多少 ?

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS9qgfMnYZ1SmBMAsOXXib6oag899Jt4tHoPgkG6opHLJWUrutMefWtjnIHPibcLmnM3Wbe0ZBLGiavOg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

A、7

B、30

C、35

D、Can’t Say



正确答案是： B

解析：

答案: B主成分选择使variance越大越好， 在这个前提下， 主成分越少越好。

## 530题

数据科学家可能会同时使用多个算法（模型）进行预测， 并且最后把这些算法的结果集成起来进行最后的预测（集成学习），以下对集成学习说法正确的是

A、单个模型之间有高相关性

B、单个模型之间有低相关性

C、在集成学习中使用“平均权重”而不是“投票”会比较好

D、单个模型都是用的一个算法



正确答案是： B

解析：

答案: B 

详细请参考下面文章:Basics of Ensemble Learning Explained in Simple EnglishKaggle Ensemble Guide5 Easy questions on Ensemble Modeling everyone should know

## 531题

在有监督学习中， 我们如何使用聚类方法？ 

1) 我们可以先创建聚类类别， 然后在每个类别上用监督学习分别进行学习

2) 我们可以使用聚类“类别id”作为一个新的特征项， 然后再用监督学习分别进行学习

3) 在进行监督学习之前， 我们不能新建聚类类别

4) 我们不可以使用聚类“类别id”作为一个新的特征项， 然后再用监督学习分别进行学习

A、2 和 4

B、1 和 2

C、3 和 4

D、 1 和 3



正确答案是： B

解析：

答案: B我们可以为每个聚类构建不同的模型， 提高预测准确率。“类别id”作为一个特征项去训练， 可以有效地总结了数据特征。所以B是正确的

## 532题

以下说法正确的是

1) 一个机器学习模型，如果有较高准确率，总是说明这个分类器是好的

2) 如果增加模型复杂度， 那么模型的测试错误率总是会降低

3) 如果增加模型复杂度， 那么模型的训练错误率总是会降低

4) 我们不可以使用聚类“类别id”作为一个新的特征项， 然后再用监督学习分别进行学习

A、1

B、2

C、3

D、2和3

E、都错



正确答案是：E

解析：

答案:E1的模型中, 如果负样本占比非常大,也会有很高的准确率, 对正样本的分类不一定很好;4的模型中, “类别id”可以作为一个特征项去训练, 这样会有效地总结了数据特征。

## 533题

对应GradientBoosting tree算法， 以下说法正确的是:

1) 当增加最小样本分裂个数，我们可以抵制过拟合 

2) 当增加最小样本分裂个数，会导致过拟合

3) 当我们减少训练单个学习器的样本个数，我们可以降低variance 

4) 当我们减少训练单个学习器的样本个数，我们可以降低bias

A、2 和 4

B、2 和 3

C、1 和 3

D、1 和 4



正确答案是：C

解析：

答案: C最小样本分裂个数是用来控制“过拟合”参数。太高的值会导致“欠拟合”，这个参数应该用交叉验证来调节。 

第二点是靠bias和variance概念的。

## 534题

以下哪个图是KNN算法的训练边界

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpSicNXibMnR7UOcy7Wmllkhklduh3SbrerbVyf8miaOvr9Det9zzkcWfQuyvNZZ1mC5ndT2b7KfzczUeA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

A、B

B、A

C、D

D、C

E、都不是



正确答案是： B

解析：

答案: BKNN算法肯定不是线性的边界， 所以直的边界就不用考虑了。另外这个算法是看周围最近的k个样本的分类用以确定分类，所以边界一定是坑坑洼洼的。

## 535题

如果一个训练好的模型在测试集上有100%的准确率， 这是不是意味着在一个新的数据集上，也会有同样好的表现？

A、是的，这说明这个模型的范化能力已经足以支持新的数据集合了

B、不对，依然后其他因素模型没有考虑到，比如噪音数据



正确答案是： B

解析：

答案: B没有一个模型是可以总是适应新数据的。我们不可能可到100%准确率。

## 536题

下面的交叉验证方法

i. 有放回的Bootstrap方法 

ii. 留一个测试样本的交叉验证 

iii. 5折交叉验证 

iv. 重复两次的5折教程验证当样本是1000时，下面执行时间的顺序，正确的是

A、i > ii > iii > iv

B、ii > iv > iii > i

C、iv > i > ii > iii

D、ii > iii > iv > i



正确答案是： B

解析：

答案: B 

Boostrap方法是传统地随机抽样，验证一次的验证方法，只需要训练1次模型，所以时间最少。留一个测试样本的交叉验证，需要n次训练过程（n是样本个数），这里，要训练1000个模型。 

5折交叉验证需要训练5个模型。 

重复2次的5折交叉验证，需要训练10个模型。 

所以B是正确的

## 537题

变量选择是用来选择最好的判别器子集， 如果要考虑模型效率，我们应该做哪些变量选择的考虑？

1 多个变量其实有相同的用处

2 变量对于模型的解释有多大作用

3 特征携带的信息 

4 交叉验证

A、1 和 4

B、1, 2 和 3

C、1,3 和 4

D/以上所有



正确答案是：C

解析：

答案: C注意， 这题的题眼是考虑模型效率，所以不要考虑选项2.

## 538题

对于线性回归模型，包括附加变量在内，以下的可能正确的是 :

1 R-Squared 和 Adjusted R-squared都是递增的 

2 R-Squared 是常量的，Adjusted R-squared是递增的 

3 R-Squared 是递减的， Adjusted R-squared 也是递减的 

4 R-Squared 是递减的， Adjusted R-squared是递增的

A、1 和 2

B、1 和 3

C、2 和 4

D、以上都不是



正确答案是：D

解析：

答案: DR-squared不能决定系数估计和预测偏差，这就是为什么我们要估计残差图。但是，R-squared有R-squared 和 predicted R-squared 所没有的问题。每次你为模型加入预测器，R-squared递增或不变.


## 539题

对于下面三个模型的训练情况， 下面说法正确的是:

![null](https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpS8P1JqhiaHTLBYicvbvrzHqAfH8zCcqJn7yqAvEtZzVZZpgiakriaBkClUYgTH9RcFkSUiabVZ3qLVf92g/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

1 第一张图的训练错误与其余两张图相比，是最大的

2 最后一张图的训练效果最好，因为训练错误最小

3 第二张图比第一和第三张图鲁棒性更强，是三个里面表现最好的模型 

4 第三张图相对前两张图过拟合了 

5 三个图表现一样，因为我们还没有测试数据集

A、1 和 3

B、1 和 3

C、1, 3 和 4

D、5



正确答案是：C

解析：

答案: C最后一张过拟合, 训练错误最小, 第一张相反, 训练错误就是最大了. 所以1是对的;仅仅训练错误最小往往说明过拟合, 所以2错, 4对;第二张图平衡了拟合和过拟合, 所以3对;

## 540题

对于线性回归，我们应该有以下哪些假设？

1 找到离群点很重要, 因为线性回归对离群点很敏感

2 线性回归要求所有变量必须符合正态分布

3 线性回归假设数据没有多重线性相关性

A、1 和 2

B、2 和 3

C、1,2 和 3

D、以上都不是



正确答案是：D

解析：

答案: D 

第1个假设, 离群点要着重考虑, 第一点是对的 

第2个假设, 正态分布不是必须的. 当然, 如果是正态分布, 训练效果会更好 

第3个假设, 有少量的多重线性相关性也是可以的, 但是我们要尽量避免

## 541题

当我们构造线性模型时, 我们注意变量间的相关性. 在相关矩阵中搜索相关系数时, 如果我们发现3对变量的相关系数是(Var1 和Var2, Var2和Var3, Var3和Var1)是-0.98, 0.45, 1.23 . 我们可以得出什么结论: 

1)Var1和Var2是非常相关的

2) 因为Var1和Var2是非常相关的, 我们可以去除其中一个


3) Var3和Var1的1.23相关系数是不可能的

A、1 and 3

B、1 and 2

C、1,2 and 3

D、1



正确答案是：C

解析：

答案: C 

相关性系数范围应该是 [-1,1]一般地, 如果相关系数大于0.7或者小于-0.7, 是高相关的.Var1和Var2相关系数是接近负1, 所以这是多重线性相关, 我们可以考虑去除其中一个. 所以1, 2, 3个结论都是对的, 选C.

## 542题

如果在一个高度非线性并且复杂的一些变量中, 一个树模型可能比一般的回归模型效果更好. 这是（）

A、对的

B、错的



正确答案是：A

解析：

答案: A

## 543题

下面对集成学习模型中的弱学习者描述错误的是？

A、他们经常不会过拟合

B、他们通常带有高偏差，所以其并不能解决复杂学习问题

C、他们通常会过拟合



正确答案是：C

解析：

答案：C，弱学习者是问题的特定部分。所以他们通常不会过拟合，这也就意味着弱学习者通常拥有低方差和高偏差。

## 544题

下面哪个/些选项对 K 折交叉验证的描述是正确的？

1)增大 K 将导致交叉验证结果时需要更多的时间

2)更大的 K 值相比于小 K 值将对交叉验证结构有更高的信心 

3)如果 K=N，那么其称为留一交叉验证，其中 N 为验证集中的样本数量

A、1 和 2

B、2 和 3

C、1 和 3

D、1、2 和 3



正确答案是：D

解析：

答案（D)：大 K 值意味着对过高估计真实预期误差（训练的折数将更接近于整个验证集样本数）拥有更小的偏差和更多的运行时间（并随着越来越接近极限情况：留一交叉验证）。我们同样在选择 K 值时需要考虑 K 折准确度和方差间的均衡。

## 545题

最出名的降维算法是 PCA 和 t-SNE。将这两个算法分别应用到数据「X」上，并得到数据集「X_projected_PCA」，「X_projected_tSNE」。下面哪一项对「X_projected_PCA」和「X_projected_tSNE」的描述是正确的？

A、X_projected_PCA 在最近邻空间能得到解释

B、X_projected_tSNE 在最近邻空间能得到解释

C、两个都在最近邻空间能得到解释

D、两个都不能在最近邻空间得到解释



正确答案是： B

解析：

答案（B）：t-SNE 算法考虑最近邻点而减少数据维度。所以在使用 t-SNE 之后，所降的维可以在最近邻空间得到解释。但 PCA 不能。

## 546题

给定三个变量 X，Y，Z。(X, Y)、(Y, Z) 和 (X, Z) 的 Pearson 相关性系数分别为 C1、C2 和 C3。现在 X 的所有值加 2（即 X+2），Y 的全部值减 2（即 Y-2），Z 保持不变。那么运算之后的 (X, Y)、(Y, Z) 和 (X, Z) 相关性系数分别为 D1、D2 和 D3。现在试问 D1、D2、D3 和 C1、C2、C3 之间的关系是什么？

A、D1= C1, D2 < C2, D3 > C3

B、D1 = C1, D2 > C2, D3 > C3

C、D1 = C1, D2 > C2, D3 < C3

D、D1 = C1, D2 < C2, D3 < C3

E、D1 = C1, D2 = C2, D3 = C3



正确答案是：E

解析：

答案（E）：特征之间的相关性系数不会因为特征加或减去一个数而改变。

## 547题

为了得到和 SVD 一样的投射（projection），你需要在 PCA 中怎样做？

A、将数据转换成零均值

B、将数据转换成零中位数

C、无法做到



正确答案是：A

解析：

答案（A）：当数据有一个 0 均值向量时，PCA 有与 SVD 一样的投射，否则在使用 SVD 之前，你必须将数据均值归 0。

## 548题

假设我们有一个数据集，在一个深度为 6 的决策树的帮助下，它可以使用 100% 的精确度被训练。现在考虑一下两点，并基于这两点选择正确的选项。注意：所有其他超参数是相同的，所有其他因子不受影响。 

1)深度为 4 时将有高偏差和低方差

2)深度为 4 时将有低偏差和低方差

A、只有 1

B、只有 2

C、1 和 2

D、没有一个



正确答案是：A

解析：

答案（A)：如果在这样的数据中你拟合深度为 4 的决策树，这意味着其更有可能与数据欠拟合。因此，在欠拟合的情况下，你将获得高偏差和低方差。

## 549题

在以下不同的场景中,使用的分析方法不正确的有

A、根据商家最近一年的经营及服务数据,用聚类算法判断出天猫商家在各自主营类目下所属的商家层级

B、根据商家近几年的成交数据,用聚类算法拟合出用户未来一个月可能的消费金额公式

C、用关联规则算法分析出购买了汽车坐垫的买家,是否适合推荐汽车脚垫

D、根据用户最近购买的商品信息,用决策树算法识别出淘宝买家可能是男还是女



正确答案是： B

解析：

解析：预测消费更合适的算法是用回归模型来做。而不是聚类算法。

## 550题

以下对k-means聚类算法解释正确的是

A、能自动识别类的个数,随即挑选初始点为中心点计算

B、能自动识别类的个数,不是随即挑选初始点为中心点计算

C、不能自动识别类的个数,随即挑选初始点为中心点计算

D、不能自动识别类的个数,不是随即挑选初始点为中心点计算



正确答案是：C

解析： 

（1）适当选择c个类的初始中心； 

（2）在第k次迭代中，对任意一个样本，求其到c个中心的距离，将该样本归到距离最短的中心所在的类； 

（3）利用均值等方法更新该类的中心值； 

（4）对于所有的c个聚类中心，如果利用（2）（3）的迭代法更新后，值保持不变，则迭代结束，否则继续迭代。 

以上是KMeans（C均值）算法的具体步骤，可以看出需要选择类别数量，但初次选择是随机的，最终的聚类中心是不断迭代稳定以后的聚类中心。所以答案选C。

## 551题

（假设precision=TP/(TP+FP),recall=TP/(TP+FN)。）在二分类问题中，当测试集的正例和负例数量不均衡时，以下评价方案哪个是相对不合理的（）

A、Accuracy:(TP+TN)/all

B、F-value:2*recall*precision/(recall+precision)

C、G-mean:sqrt(precision*recall)

D、AUC:曲线下面积



正确答案是：A

解析：题目提到测试集正例和负例数量不均衡，那么假设正例数量很少占10%，负例数量占大部分90%。而且算法能正确识别所有负例，但正例只有一半能正确判别。那么TP=0.05×all,TN=0.9×all，Accuracy=95%。虽然Accuracy很高，precision是100%,但正例recall只有50%

## 552题

下列选项中,识别模式与其他不⼀样的是

A、⽤户年龄分布判断:少年、青年、中年、⽼年

B、医⽣给病⼈诊断发病类型

C、投递员分拣信件

D、消费者类型判断:⾼消费、⼀般消息、低消费

E、出⾏方式判断:步⾏、骑车、坐车

F、商家对商品分级



正确答案是：E

解析：

解析：E属于预测问题，其他的选项属于分类问题

## 553题

在大规模的语料中，挖掘词的相关性是一个重要的问题。以下哪一个信息不能用于确定两个词的相关性。

A、互信息

B、最大熵

C、卡方检验

D、最大似然比



正确答案是： B

解析：

解析：最大熵代表了整体分布的信息，通常具有最大熵的分布作为该随机变量的分布，不能体现两个词的相关性，但是卡方是检验两类事务发生的相关性。

所以选B【正解】

## 554题

基于统计的分词方法为（）

A、正向最大匹配法

B、逆向最大匹配法

C、最少切分

D、条件随机场



正确答案是：D

解析：

第一类是基于语法和规则的分词法。其基本思想就是在分词的同时进行句法、语义分析,利用句法信息和语义信息来进行词性标注,以解决分词歧义现象。因为现有的语法知识、句法规则十分笼统、复杂,基于语法和规则的分词法所能达到的精确度远远还不能令人满意,目前这种分词系统还处在试验阶段。 

第二类是机械式分词法（即基于词典）。机械分词的原理是将文档中的字符串与词典中的词条进行逐一匹配,如果词典中找到某个字符串,则匹配成功,可以切分,否则不予切分。基于词典的机械分词法,实现简单,实用性强,但机械分词法的最大的缺点就是词典的完备性不能得到保证。据统计,用一个含有70000个词的词典去切分含有15000个词的语料库,仍然有30%以上的词条没有被分出来,也就是说有4500个词没有在词典中登录。 

第三类是基于统计的方法。基于统计的分词法的基本原理是根据字符串在语料库中出现的统计频率来决定其是否构成词。词是字的组合,相邻的字同时出现的次数越多,就越有可能构成一个词。因此字与字相邻共现的频率或概率能够较好的反映它们成为词的可信度。

## 555题

在下面的图像中，哪一个是多元共线（multi-collinear）特征？

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibEkfibvxcOLXHHNtqUNAxh9UqicvH7TfZlHGDZl38UgtAKKhLCT6g1HB9ewVYwdhUrc96KP0ZzoBUA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

A、图 1 中的特征

B、图 2 中的特征

C、图 3 中的特征

D、图 1、2 中的特征

E、图 2、3 中的特征

F、图 1、3 中的特征



正确答案是：D

解析：

答案为（D）：在图 1 中，特征之间有高度正相关，图 2 中特征有高度负相关。所以这两个图的特征是多元共线特征。

## 556题

线性回归的基本假设不包括哪个？

A、随机误差项是一个期望值为0的随机变量

B、对于解释变量的所有观测值，随机误差项有相同的方差

C、随机误差项彼此相关

D、解释变量是确定性变量不是随机变量，与随机误差项之间相互独立

E、随机误差项服从正态分布



正确答案是：C

## 557题

下面哪些对「类型 1（Type-1）」和「类型 2（Type-2）」错误的描述是错误的？

A、类型 1 通常称之为假正类，类型 2 通常称之为假负类

B、类型 2 通常称之为假正类，类型 1 通常称之为假负类

C、类型 1 错误通常在其是正确的情况下拒绝假设而出现



正确答案是： B

解析：

在统计学假设测试中，I 类错误即错误地拒绝了正确的假设即假正类错误，II 类错误通常指错误地接受了错误的假设即假负类错误。

## 558题

给线性回归模型添加一个不重要的特征可能会造成？

A、增加 R-square

B、减少 R-square



正确答案是：A

解析：

答案为（A）：在给特征空间添加了一个特征后，不论特征是重要还是不重要，R-square 通常会增加。R-square定义如下:

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS9yCOTLaC5aITvBBhfk9BbJdgXnLcjL3TkjYRkXpCJaPnB0jtiaQhwn0zYuCd8Y6D1Ur8yzE7QLq9A/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

在给特征空间添加了一个特征后，分子会增加一个残差平方项, 分母会增加一个均值差平方项, 前者一般小于后者, 所以不论特征是重要还是不重要，R-square 通常会增加。

## 559题

关于 ARMA 、 AR 、 MA 模型的功率谱，下列说法正确的是（ ）

A、MA模型是同一个全通滤波器产生的

B、MA模型在极点接近单位圆时，MA谱是一个深谷

C、AR模型在零点接近单位圆时，AR谱是一个尖峰

D、RMA谱既有尖峰又有深谷



正确答案是：D

## 560题

符号集 a 、 b 、 c 、 d ，它们相互独立，相应概率为 1/2 、 1/4 、 1/8/ 、 1/16 ，其中包含信息量最小的符号是（ ）

A、a

B、b

C、c

D、d



正确答案是：A

解析：

因为消息出现的概率越小，则消息中所包含的信息量就越大。因此选a,同理d信息量最大。

## 561题

下列哪个不属于常用的文本分类的特征选择算法？

A、卡方检验值

B、互信息

C、信息增益

D、主成分分析



正确答案是：D

解析：

主成分分析是特征转换算法（特征抽取），而不是特征选择

## 562题

在数据清理中，下面哪个不是处理缺失值的方法?

A、估算

B、整例删除

C、变量删除

D、成对删除



正确答案是：D

解析：

数据清理中，处理缺失值的方法有两种： 

一、删除法：

1）删除观察样本 

2）删除变量：当某个变量缺失值较多且对研究目标影响不大时，可以将整个变量整体删除 

3）使用完整原始数据分析：当数据存在较多缺失而其原始数据完整时，可以使用原始数据替代现有数据进行分析 

4）改变权重：当删除缺失数据会改变数据结构时，通过对完整数据按照不同的权重进行加权，可以降低删除缺失数据带来的偏差

二、查补法：均值插补、回归插补、抽样填补等

## 563题

统计模式分类问题中，当先验概率未知时，可以使用（）

A、最小最大损失准则

B、最小误判概率准则

C、最小损失准则

D、N-P判决



正确答案是：A

解析：

A. 考虑p(wi)变化的条件下，是风险最小 

B. 最小误判概率准则， 就是判断p(w1|x)和p(w2|x)哪个大，x为特征向量，w1和w2为两分类，根据贝叶斯公式，需要用到先验知识 

C. 最小损失准则，在B的基础之上，还要求出p(w1|x)和p(w2|x)的期望损失，因为B需要先验概率，所以C也需要先验概率 

D. N-P判决，即限定一类错误率条件下使另一类错误率为最小的两类别决策，即在一类错误率固定的条件下，求另一类错误率的极小值的问题，直接计算p(x|w1)和p(x|w2)的比值，不需要用到贝叶斯公式_

## 564题

决策树的父节点和子节点的熵的大小关系是什么？

A、A. 决策树的父节点更大

B、B. 子节点的熵更大

C、C. 两者相等

D、D. 根据具体情况而定



正确答案是：D

解析：

正确答案：D。 

假设一个父节点有2正3负样本，进一步分裂

情况1：两个叶节点（2正，3负）；

情况2：两个叶节点（1正1负，1正2负）。

分别看下情况1和情况2，分裂前后确实都有信息增益，但是两种情况里不是每一个叶节点都比父节点的熵小。

## 565题

语言模型的参数估计经常使用MLE（最大似然估计）。面临的一个问题是没有出现的项概率为0，这样会导致语言模型的效果不好。为了解决这个问题，需要使用（ ）

A、平滑

B、去噪

C、随机插值

D、增加白噪音



正确答案是：A

解析：

A，拉普拉斯平滑假设，将分子和分母各加上一个常数项。

## 566题

逻辑回归与多元回归分析有哪些不同？

A、逻辑回归预测某事件发生的概率

B、逻辑回归有较高的拟合效果

C、逻辑回归回归系数的评估

D、以上全选



正确答案是：D

解析：

答案：D 

逻辑回归是用于分类问题，我们能计算出一个事件/样本的概率；一般来说，逻辑回归对测试数据有着较好的拟合效果；建立逻辑回归模型后，我们可以观察回归系数类标签(正类和负类)与独立变量的的关系。

## 567题

"过拟合是有监督学习的挑战，而不是无监督学习"以上说法是否正确：

A、正确

B、错误



正确答案是： B

解析：

答案：B 

我们可以评估无监督学习方法通过无监督学习的指标，如：我们可以评估聚类模型通过调整兰德系数。

## 568题

中文同义词替换时，常用到Word2Vec，以下说法错误的是

A、Word2Vec基于概率统计

B、Word2Vec结果符合当前预料环境

C、Word2Vec得到的都是语义上的同义词

D、Word2Vec受限于训练语料的数量和质量



正确答案是：C

解析：

Word2vec，为一群用来产生词向量的相关模型。这些模型为浅而双层的神经网络，用来训练以重新建构语言学之词文本。网络以词表现，并且需猜测相邻位置的输入词，在word2vec中词袋模型假设下，词的顺序是不重要的。 

训练完成之后，word2vec模型可用来映射每个词到一个向量，可用来表示词对词之间的关系。该向量为神经网络之隐藏。Word2vec依赖skip-grams或连续词袋（CBOW）来建立神经词嵌入。

## 569题

假定你用一个线性SVM分类器求解二类分类问题，如下图所示，这些用红色圆圈起来的点表示支持向量

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSicKcWXjC58Nr8hCGY2yVW6Oia2flNKcVOhH67t26CsQWsN5Jjia6SVtUNdAibWxicJozGe8QE8WqzPyzA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

如果移除这些圈起来的数据，决策边界（即分离超平面）是否会发生改变？

A、Yes

B、No



正确答案是： B

解析：

从数据的分布来看，移除那三个数据，决策边界不会受影响。

## 570题

如果将数据中除圈起来的三个点以外的其他数据全部移除，那么决策边界是否会改变？

![null](https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSicKcWXjC58Nr8hCGY2yVW6Oia2flNKcVOhH67t26CsQWsN5Jjia6SVtUNdAibWxicJozGe8QE8WqzPyzA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

A、会

B、不会



正确答案是： B

解析：

决策边界只会被支持向量影响，跟其他点无关。

## 571题

关于SVM泛化误差描述正确的是

A、超平面与支持向量之间距离

B、SVM对未知数据的预测能力

C、SVM的误差阈值



正确答案是： B

解析：

统计学中的泛化误差是指对模型对未知数据的预测能力。

## 572题

以下关于硬间隔hard margin描述正确的是

A、SVM允许分类存在微小误差

B、SVM允许分类是有大量误差



正确答案是：A

解析：

硬间隔意味着SVM在分类时很严格，在训练集上表现尽可能好，有可能会造成过拟合。

## 573题

训练SVM的最小时间复杂度为O(n2)，那么一下哪种数据集不适合用SVM?

A、大数据集

B、小数据集

C、中等大小数据集

D、和数据集大小无关



正确答案是：A

解析：

有明确分类边界的数据集最适合SVM

## 574题

SVM的效率依赖于

A、核函数的选择

B、核参数

C、软间隔参数

D、以上所有



正确答案是：D

解析：

SVM的效率依赖于以上三个基本要求，它能够提高效率，降低误差和过拟合

## 575题

支持向量是那些最接近决策平面的数据点

A、对

B、错



正确答案是：A

解析：

支持向量就在间隔边界上

## 576题

SVM在下列那种情况下表现糟糕

A、线性可分数据

B、清洗过的数据

C、含噪声数据与重叠数据点



正确答案是：C

解析：

当数据中含有噪声数据与重叠的点时，要画出干净利落且无误分类的超平面很难

## 577题

假定你使用了一个很大γ值的RBF核，这意味着：

A、模型将考虑使用远离超平面的点建模

B、模型仅使用接近超平面的点来建模

C、模型不会被点到超平面的距离所影响

D、以上都不正确



正确答案是： B

解析：

SVM调参中的γ衡量距离超平面远近的点的影响。对于较小的γ，模型受到严格约束，会考虑训练集中的所有点，而没有真正获取到数据的模式、对于较大的γ，模型能很好地学习到模型。

## 578题

SVM中的代价参数表示：

A、交叉验证的次数

B、使用的核

C、误分类与模型复杂性之间的平衡

D、以上均不是



正确答案是：C

解析：

代价参数决定着SVM能够在多大程度上适配训练数据。如果你想要一个平稳的决策平面，代价会比较低；如果你要将更多的数据正确分类，代价会比较高。可以简单的理解为误分类的代价。

## 579题

假定你使用SVM学习数据X，数据X里面有些点存在错误。现在如果你使用一个二次核函数，多项式阶数为2，使用松弛变量C作为超参之一。当你使用较大的C（C趋于无穷），则：

A、仍然能正确分类数据

B、不能正确分类

C、不确定

D、以上均不正确



正确答案是：A

解析：

采用更大的C，误分类点的惩罚就更大，因此决策边界将尽可能完美地分类数据。

## 580题

如果我使用数据集的全部特征并且能够达到100%的准确率，但在测试集上仅能达到70%左右，这说明：

A、欠拟合

B、模型很棒

C、过拟合



正确答案是：C

解析：

如果在训练集上模型很轻易就能达到100%准确率，就要检查是否发生过拟合。

## 581题

假定你使用SVM学习数据X，数据X里面有些点存在错误。现在如果你使用一个二次核函数，多项式阶数为2，使用松弛变量C作为超参之一。如果使用较小的C（C趋于0），则：

A、误分类

B、正确分类

C、不确定

D、以上均不正确



正确答案是：A

解析：

分类器会最大化大多数点之间的间隔，少数点会误分类，因为惩罚太小了。

## 582题

下面哪个属于SVM应用

A、文本和超文本分类

B、图像分类

C、新文章聚类

D、以上均是



正确答案是：D

解析：

SVM广泛应用于实际问题中，包括回归，聚类，手写数字识别等。

## 583题

假设你训练SVM后，得到一个线性决策边界，你认为该模型欠拟合。在下次迭代训练模型时，应该考虑：

A、增加训练数据

B、减少训练数据

C、计算更多变量

D、减少特征



正确答案是：C

解析：

由于是欠拟合，最好的选择是创造更多特征带入模型训练。

## 584题

假设你训练SVM后，得到一个线性决策边界，你认为该模型欠拟合。假如你想修改SVM的参数，同样达到模型不会欠拟合的效果，应该怎么做？

A、增大参数C

B、减小参数C

C、改变C并不起作用

D、以上均不正确



正确答案是：A

解析：

增大参数C会得到正则化模型

## 585题

SVM中使用高斯核函数之前通常会进行特征归一化，以下关于特征归一化描述不正确的是？

A、经过特征正则化得到的新特征优于旧特征

B、特征归一化无法处理类别变量

C、SVM中使用高斯核函数时，特征归一化总是有用的



正确答案是：C

解析：

非万能。

## 586题

假设现在只有两个类，这种情况下SVM需要训练几次？

A、1

B、2

C、3

D、4



正确答案是：A

解析：

两个类训练1次就可以了

## 587题

假设你训练了一个基于线性核的SVM，多项式阶数为2，在训练集和测试集上准确率都为100%。如果增加模型复杂度或核函数的多项式阶数，将会发生什么？

A、导致过拟合

B、导致欠拟合

C、无影响，因为模型已达100%准确率

D、以上均不正确



正确答案是：A

解析：

增加模型复杂度会导致过拟合

## 588题

想象一下，机器学习中有1000个输入特征和1个目标特征，必须根据输入特征和目标特征之间的关系选择100个最重要的特征。你认为这是减少维数的例子吗？

A、是

B、不是



正确答案是：A

## 589题

判断：没有必要有一个用于应用维数降低算法的目标变量。

A、真

B、假



正确答案是：A

解析：

LDA是有监督降维算法的一个例子。

## 590题

在数据集中有4个变量，如A，B，C和D.执行了以下操作：步骤1：使用上述变量创建另外两个变量，即E = A + 3 \* B和F = B + 5 \* C + D。步骤2：然后只使用变量E和F建立了一个随机森林模型。 上述步骤可以表示降维方法吗？

A、真

B、假



正确答案是：A

解析：

因为步骤1可以用于将数据表示为2个较低的维度。

## 591题

以下哪种技术对于减少数据集的维度会更好？

A、删除缺少值太多的列

B、删除数据差异较大的列

C、删除不同数据趋势的列

D、都不是



正确答案是：A

解析：

如果列的缺失值太多（例如99％），那么可以删除这些列。

## 592题

判断：降维算法是减少构建模型所需计算时间的方法之一。

A、真

B、假



正确答案是：A

解析：

降低数据维数将花费更少的时间来训练模型。

## 593题

以下哪种算法不能用于降低数据的维数？

A、t-SNE

B、PCA

C、LDA

D、都不是



正确答案是：D

解析：

所有算法都是降维算法的例子。

## 594题

判断：PCA可用于在较小维度上投影和可视化数据。

A、真

B、假



正确答案是：A

解析：

有时绘制较小维数据非常有用，可以使用前两个主要分量，然后使用散点图可视化数据。

## 595题

最常用的降维算法是PCA，以下哪项是关于PCA的？

1)PCA是一种无监督的方法

2)它搜索数据具有最大差异的方向

3)主成分的最大数量<=特征能数量

4)所有主成分彼此正交

A、2、3和4

B、1、2和3

C、1、2和4

D、以上所有



正确答案是：D

## 596题

假设使用维数降低作为预处理技术，使用PCA将数据减少到k维度。然后使用这些PCA预测作为特征，以下哪个声明是正确的？

A、更高的“k”意味着更正则化

B、更高的“k”意味着较少的正则化

C、都不对



正确答案是： B

解析：

较高的k导致较少的平滑，因此能够保留更多的数据特征，从而减少正则化。

## 597题

在相同的机器上运行并设置最小的计算能力，以下哪种情况下t-SNE比PCA降维效果更好？

A、具有1百万项300个特征的数据集

B、具有100000项310个特征的数据集

C、具有10,000项8个特征的数据集

D、具有10,000项200个特征的数据集



正确答案是：C

解析：

t-SNE具有二次时空复杂度。

## 598题

对于t-SNE代价函数，以下陈述中的哪一个正确？

A、本质上是不对称的

B、本质上是对称的

C、与SNE的代价函数相同



正确答案是： B

解析：

SNE代价函数是不对称的，这使得使用梯度下降难以收敛。对称是SNE和t-SNE代价函数之间的主要区别之一。

## 599题

想像正在处理文本数据，使用单词嵌入（Word2vec）表示使用的单词。在单词嵌入中，最终会有1000维。现在想减小这个高维数据的维度，这样相似的词应该在最邻近的空间中具有相似的含义。在这种情况下，您最有可能选择以下哪种算法？

A、t-SNE

B、PCA

C、LDA

D、都不是



正确答案是：A

解析：

t-SNE代表t分布随机相邻嵌入，它考虑最近的邻居来减少数据。

## 600题

判断：t-SNE学习非参数映射。

A、真

B、假



正确答案是：A

解析：

t-SNE学习非参数映射，这意味着它不会学习将数据从输入空间映射到地图的显式函数。
