# 修改一个像素，就能让神经网络识别图像出错

选自arXiv

**作者：Su Jiawei等人**

[**机器之心编辑部**](http://www.sohu.com/a/200331224_465975)

> 用于识别图片中物体的神经网络可以被精心设计的对抗样本欺骗，这个问题目前在计算机视觉领域备受关注。此前，生成对抗样本通常需要向原图片中加入一些特定的噪点（参见：经得住考验的「假图片」：用 TensorFlow 为神经网络生成对抗样本）。然而最近，日本九州大学的 Su Jiawei 等人发表的研究证明：修改图片中的一个像素也可以让深度神经网络完全判断错误。

![](https://img.hacpai.com/e/b53ae760519c4a03a18efae38c028c8f.png)

在图像识别领域，基于 DNN 的方法突破了传统的图像处理技术，达到了可与人类媲美的结果 \[9\]。但是，多个研究证明自然图像中的人工扰动可以轻易使 DNN 对图像进行错误分类，研究者提出了生成此类「对抗图像」的高效算法 \[1, 2, 3, 4\]。生成对抗图像（adversarial images）的主要方式是向准确分类的自然图像中添加精心设计的额外扰动，该扰动不影响人类对图像的识别。这样的修改导致分类器将修改后的图像标注为完全不同的其他物体。但是，大多数之前的攻击并未考虑非常有限的对抗实例，即扰动的量有时候也会影响到人眼的识别能力（示例见图 2）。此外，研究在有限场景中创建的对抗图像更加有趣，因为它们可能更接近原始类别和目标类别（target class）之间的边界，研究此类关键点可以使人类更多地了解 DNN 输入空间的几何特征 \[23\]。

![](https://img.hacpai.com/e/a021b59dfc2141278fa76b8938cdc951.jpeg)

图 1\. 使用该研究算法生成的对抗图像成功地在只更改一个像素的情况下误导了目标 DNN。括号内的标签为识别类别，括号外的标签是原类别。修改的像素可能不是很明显，读者需要仔细查看。

在这篇论文中，通过使用差分进化（differential evolution）扰乱少数像素（1024 个像素中只扰乱 1、3 或 5 个像素）的方式，研究者提出了黑箱 DNN 攻击，研究者将其称为「少量像素攻击」（few-pixels attack），该场景中仅需获取标签的概率信息。研究者提出的方法与之前的研究相比优势在于：

*   能够在仅仅修改 1、3 或 5 个像素的情况下发起非靶向攻击，成功率分别为 73.8%、82.0% 和 87.3%，预测目标类别的标签置信概率平均达到了 98.7%。
    
*   仅需要黑箱反馈（标签概率），无需目标 DNN 的内部信息，如梯度和网络结构。研究者的方法比较简单，因为它不需要将搜索扰动的问题形式化为任何精确的目标函数，而是直接聚焦于提高目标类别的标签概率值。
    
*   能够攻击更多类型的DNN（如网络不可微或梯度计算比较难的情况）。
    

这篇论文的作者认为少数像素攻击的实现有两大原因。1）实践中，少数像素攻击可以有效隐藏修改。之前的研究无法保证扰动完全不可见。解决这个问题的一个直接方法就是尽量限制扰动的幅度。具体来说，就是不使用理论提出的额外约束或考虑更复杂的扰动成本函数，而是控制修改的像素数量，如在一个 32 X 32 的图像中修改 1、3 和 5 个像素，即使用像素数量而不是扰动向量的长度来衡量扰动的强度。2）从几何的角度来看，多个之前的研究通过限制像素修改来分析自然图像的周围（vicinity）。例如，通用扰动向每个像素添加小的值，使之在自然图像周围的球形区域中搜索对抗图像 \[24\]。另外，少量像素扰动可以使用维度非常低的子空间来削减输入空间，这也是一种探索 DNN 输入空间特征的不同方法。

根据实验结果，该研究的主要贡献包括：

*   使用少数像素扰动能高效地实施非靶向攻击。研究发现仅需要修改 1 个像素，就可将 73.8% 的图像扰动至一个或多个目标类别，在修改 3 个像素和 5 个像素的情况下，比例分别为 82.0% 和 87.3%。这表明不敏感的图像比敏感图像更加稀少，即使扰动已经被限制在这么小的范围内。因为少数像素修改是搜索对抗图像的有效方法，而这种图像在实践中不易被人眼识别。
    
*   自然图像可以隐藏的目标类别数量。在 1 个像素扰动的情况下，平均每个自然图像可以被扰动至 2.3 个其他类别。具体来说，18.4%、17.2% 和 16.6% 的图像分别被扰动至 1、2、3 个目标类别。在 5 个像素扰动的情况下，被扰动至 1 到 9 个目标类别的图像数量几乎相同。
    

![](https://img.hacpai.com/e/9fa23869d38141f887fd9239979f194b.jpeg)

图 2\. 使用 \[1\] 生成对抗图像的图示。在所有像素的 4% 以内实施扰动，该扰动可以轻易被人眼识别。由于对抗像素扰动已经成为生成对抗图像的常见方式，因此此类不正常的「噪声」可以被专业人员识别。

扰动至特定的目标类别的类似扰动方向。通用扰动的高效性证明很多图像可以通过类似方向进行扰动，这样决策边界可能会产生多样性（leak diversity）\[24\]，而该研究的结果证明同样类别的数据点更易被扰动至具备同样扰动的特定类别（即 1、3 或 5 个像素修改）。

对高维输入空间的数据点分布的几何理解。从几何角度来看，实施少数像素攻击获取的信息可作为仅使用低维子空间切分输入空间所获取的横切面（cross section）上类别标签改变的定量结果。尤其是，研究结果表明一些决策域可能很深入地通向很多不同方向，但是在这些深区域中，决策域反而很窄。也就是说，这些域可能在输入空间的不同方向上有很多狭长的扩展突触。

下文中，对抗图像的原始真正的类别叫作「原类别」（original class），DNN 识别的对抗图像类别为「目标类别」（target class），对抗图像想要欺骗的 DNN 分类器是「目标系统」（target system）。

![](https://img.hacpai.com/e/2fa0b69004284478893e0d5fa487aaef.jpeg)

图 3：在三维输入空间中（即图像有 3 个像素），使用 1 个和 2 个像素扰动攻击的示意图，绿点表示一个自然图片进行扰动。在一个像素进行扰动时，搜索空间为自然图像三条相交线，且每一条线都相互垂直，图中用红线和黑线表示。在 2 个像素的情况下，搜索空间为三个二维相交平面，且每一个平面都相互垂直，图中用蓝色平面表示。总而言之，1 个和 2 个像素攻击搜索了 3 维输入空间中 1 维和 2 维子空间内的扰动。此外，黄圈表示使用 L^p 范数正则化前面定义的搜索空间以控制总体的修正空间。相比之下，少量像素攻击可以搜索更多的区域。

![](https://img.hacpai.com/e/4110c35e37b34a4f9fce6b44370fdad2.jpeg)

图 4：这些条形图统计了分别使用 1、3 和 5 个像素扰动成功扰乱了特定数字（0 到 9）的图像数。竖轴为经过归一化的图像数量，横轴为目标类别的数量。

**论文：One pixel attack for fooling deep neural networks**

论文链接：https://arxiv.org/abs/1710.08864

摘要：最近研究表明，DNN 的输出并不是连续的，它对输入向量上的微小扰动也非常敏感，并且我们已经依据若干种方法对神经网络造成有效扰动。在本论文中，我们基于差分进化（differential evolution）提出了一种极小视觉计算的对抗性扰动（若干个像素的攻击）的全新方法。它所需的对抗性信息很少，可用于更多种类的 DNN 模型。结果表明 73.8% 的测试图像只需修改一个像素就可转化为对抗性图像，且平均置信率达 98.7%。此外，众所周知调查 DNN 的鲁棒性问题可为理解高维度输入空间中的 DNN 决策图的几何特征提供关键线索。相较于先前工作，若干个像素攻击的执行结果有助于从不同角度量化衡量和分析几何理解。
