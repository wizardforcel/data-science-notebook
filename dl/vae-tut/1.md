# 【译】变分自编码器教程 一、简介

## Tutorial on Variational Autoencoders

[Arxiv 1606.05908](https://arxiv.org/abs/1606.05908)

> 摘要
> 
> 在短短三年内，变分自编码器（VAE）已经成为复杂分布的无监督学习的最流行的方法之一。 VAE 很有吸引力，因为它们建立在标准函数近似器（神经网络）之上，并且可以用随机梯度下降进行训练。 VAE 已经展示出产生多种复杂数据的希望，包括手写数字 [1, 2]，面部 [1, 3, 4] ，门牌号 [5, 6]，CIFAR 图像 [6]，场景物理模型 [4]，分割 [7] ，以及从静态图像预测未来 [8]。本教程介绍了 VAE 背后的直觉，解释了它们背后的数学，并描述了一些经验行为。不需要变分贝叶斯方法的先验知识。

“生成建模”是机器学习的一个广泛领域，它处理分布模型`P(X)`，定义在数据点`X`上，它在一些潜在的高维空间`X`中。例如，图像是一种流行的数据，我们可以为其创建生成模型。每个“数据点”（图像）具有数千或数百万个维度（像素），并且生成模型的工作是以某种方式捕获像素之间的依赖性，例如，邻近的像素具有相似的颜色，并且被组织成对象。 “捕获”这些依赖关系的确切含义，取决于我们想要对模型做什么。一种直接的生成模型，简单允许我们以数字方式计算`P(X)`。在图像的情况下，看起来像真实图像的`X`值应该具有高概率，而看起来像随机噪声的图像应该具有低概率。然而，像这样的模型并不一定有用：知道一个图像不太可能无法帮助我们合成一个可能的图像。

相反，人们经常关心的是产生更多的例子，像那些已经存在于数据库中的例子，但并不完全相同。我们可以从原始图像数据库开始，并合成新的没有见过的图像。我们可能会采取一个像植物这样的 3D 模型的数据库，并生成更多的模型来填充视频游戏中的森林。我们可以采取手写文本，并尝试生成更多的手写文本。像这样的工具实际上可能对图形设计师有用。我们可以形式化这个设置，通过假设我们得到示例`X`，它的分布是一些未知分布`P[gt](X)`，我们的目标是学习我们可以从中采样的模型`P`，这样`P`就可能像`P[gt]`。

训练这种类型的模型一直是机器学习社区中长期存在的问题，而且大多数经典方法都有三个严重的缺点之一。首先，他们可能需要数据结构的强力假设。其次，他们可能会进行严格的近似，导致模型不理想。或者第三，他们可能依赖于计算成本昂贵的推理过程，如马尔科夫链蒙特卡洛。最近，一些工作在通过反向传播 [9]，将神经网络训练为强大的函数近似器方面，取得了巨大进步。这些进步产生了有希望的框架，可以使用基于反向传播的函数近似器来构建生成模型。

最受欢迎的此类框架之一是变分自编码器 [1, 3]，本教程的主题。这种模型的假设很弱，通过反向传播训练很快。 VAE 确实做了近似，但是这种近似引入的误差在高容量模型下可以说是很小的。这些特征使其受欢迎程度迅速提升。

本教程旨在成为 VAE 的非正式介绍，而不是关于它们的正式科学论文。它针对的是那些人，可能用于生成模型，但可能不具备 VAE 所基于的变分贝叶斯方法，和“最小描述长度”编码模型的强大背景。本教程最初是 UCB 和 CMU 的计算机视觉阅读小组的演示，因此偏向于视觉观众。感谢您的改进建议。

## 1.1 预备：潜变量模型

在训练生成模型时，维度之间的依赖性越复杂，模型训练就越困难。例如，生成手写字符图像的问题。简单地说，我们只关心数字 0-9 的建模。如果字符的左半部分包含 5 的左半部分，则右半部分不能包含 0 的右半部分，否则非常明显字符看起来不像任何真实的数字。直观地说，如果模型在为任何特定像素指定值之前首先决定生成哪个字符，它会有所帮助。这种决定被正式称为潜变量。也就是说，在我们的模型绘制任何东西之前，它首先从集合中随机采样数字值，然后确保所有笔划与该字符匹配。 被称为“潜在”，因为只给出模型产生的字符，我们不一定知道潜在变量的哪些设置产生了该字符。我们需要使用类似计算机视觉的东西来推断它。

在我们可以说我们的模型表示我们的数据集之前，我们需要确保对于数据集中的每个数据点`X`，潜变量有一个（或许多）取值，它会导致模型生成非常像`X`的东西。形式上，我们在高维空间`Z`中有一个潜变量的向量`z`，我们可以根据所定义的某些概率密度函数（PDF）`P(z)`轻松进行抽样。然后，假设我们有一系列确定性函数，在某些空间`Θ`中由矢量`θ`参数化，其中`f: Z x Θ -> X`。`f`是确定性的，但如果是`z`随机的并且`θ`是固定的，则`f(z;θ)`产生空间`X`中的随机变量。我们希望优化`θ`，以便我们可以从`P(z)`中采样`z`，并且`f(z;θ)`很有可能将像我们数据集中`X`的一样。

为了使这个概念在数学上精确，我们的目标是在整个生成过程中最大化训练集中每个`X`的概率，根据：

![](http://latex.codecogs.com/gif.latex?%20P(X)%3D%5Cint%20P(X%7Cz%3B%5Ctheta)P(z)dz.%20) (1)

这里，`f(z;θ)`已被分布`P(X|z)`取代，这使我们可以通过全概率定律，使`X`对`z`的依赖性明确。这个框架背后的直觉称为“最大似然”，如果模型可能产生训练集样本，那么它也可能产生类似的样本，并且不太可能产生不同的样本。在 VAE 中，这种输出分布的选择通常是高斯分布，即`P(X|z;θ)=N(X|f(z;θ), σ^2 * I)`。也就是说，它的均值为`f(z;θ)`，协方差为单位矩阵乘以某些标量`σ`（这是一个超参数）。这种替换对于形成直觉是必要的，即某些`z`需要产生仅仅像`X`的样本。一般而言，特别是在训练初期，我们的模型不会产生与任何特定`X`相同的输出。通过具有高斯分布，我们可以使用梯度下降（或任何其他优化技术）来增加`P(X)`，通过使一些`z`的`f(z;θ)`接近`X`，即逐渐使训练数据在生成模型下更加可能。如果`P(X|z)`是 Dirac delta 函数，这是不可能的，因为如果我们确定性地使用`f(z;θ)`就会这样！注意，输出分布不一定是高斯的：例如，如果是二项的，那么`P(X|z)`可能是由`f(z;θ)`参数化的伯努利。重要的特性就是`P(X|z)`是可计算的，并且在`θ`中是连续的。从此开始，我们将省略`f(z;θ)`中`θ`的来避免混乱。

![](https://media.arxiv-vanity.com/render-output/1009284/x1.png)

图 1：表示为图形模型的标准 VAE 模型。 注意任何结构或甚至“编码器”路径的显着缺失：可以在没有任何输入的情况下从模型中进行采样。 这里，矩形是“平板表示法”，这意味着我们可以从`z`和`X`采样`N`次，同时模型参数`θ`保持固定。
