# 拥挤场景中的端到端人物检测

![](http://upload-images.jianshu.io/upload_images/145616-bca224046bae2873.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640)

检测效果图

> 推荐阅读[英文原文](https://link.jianshu.com?t=https://arxiv.org/pdf/1506.04878.pdf)

![](http://upload-images.jianshu.io/upload_images/145616-98df2c448c42f52b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700)

*   文章地址：[《End-to-end people detection in crowded scenes》](https://link.jianshu.com?t=https://arxiv.org/abs/1506.04878) arXiv.1506.04878
*   Github：[https://github.com/Russell91/ReInspect](https://link.jianshu.com?t=https://github.com/Russell91/ReInspect)

(未经允许禁止转载，授权转载请注明出处，谢谢！)

* * *

# Abstract

目前的人物检测操作要么是以滑动窗口的方式扫描图像，或者通过分类一组离散的决策。我们提出了基于将图像解码成一组人物检测的模型。我们的系统采用一幅图像作为输入，并直接输出一组不同的检测假设。因为我们联合生成预测，所以不需要诸如非极大值抑制的公共后处理步骤。我们使用一个循环的LSTM层进行序列生成，并使用一个新的损失函数对模型进行端对端训练，该损失函数对整个检测集合起作用。我们证实了该方法在拥挤的场景中检测人这一富有挑战性的任务上的有效性。

# 1\. Introduction

在本文中，我们提出了一种用于检测图像中的对象的新架构。 我们努力实现一种端到端的方法，它接受一幅图像作为输入，并直接生成一组对象边界框作为输出。 这个任务是具有挑战性的，因为它需要区分对象与背景并正确估计不同对象的数量及其位置。这种能够直接输出预测的端到端方法将优于首先生成一组边界框，用分类器评估它们，然后对过完备检测集合执行某种形式的合并或非极大值抑制的方法。

顺序生成一组检测具有重要的优点，即通过记住先前生成的输出可以避免对同一对象的多次检测。为了控制这个生成过程，我们使用具有LSTM单元的递归神经网络。 为了产生中间表示，我们使用来自GoogLeNet的富有表现力的图像特征，这些特征作为系统的一部分进一步微调。 因此，我们的架构可以被看作是将图像的中间表示转换成一组预测对象的“解码”过程。 LSTM可以被看作是在解码步骤之间传播信息并控制下一个输出的位置的“控制器”（参见图2的概述）。 重要的是，我们的可训练端到端系统允许通过反向传播联合调谐所有组件。

在\[6,17\]中使用的合并和非极大值抑制的主要限制之一是这些方法通常不访问图像信息，而是仅基于边界框的属性（例如距离和重叠 ）。 这通常适用于孤立的对象，但在对象实例重叠时常常失效。 在实例重叠的情况下，需要图像信息来决定将边界框放置在哪里以及输出多少个。作为解决方法，几种方法提出了专门处理预定义相关对象（例如行人对）的特定解决方案\[5,23\]。在这里，我们提出一个通用的架构，不需要对象相关的专门定义，不局限于对象对，而且是完全可训练的。

我们专注于人检测的任务作为这个问题的一个重要例子。 在拥挤场景中，例如图1所示，多个人经常出现在非常接近的地方，这使得区分附近的个体变得特别具有挑战性。

图1：OverFeat（a）和后处理（b）输出的初始过完备集合检测。 注意未能检测到中心的第三人。 (c)是用我们的方法获得的检测结果。

本文的关键贡献是一种联合预测图像中的对象的可训练的端到端方法。这与现有方法相反，现有方法将每个边界框的预测或分类视为独立问题并需要对检测集合进行后处理。 我们证明了我们的方法在富有挑战性的包含有大量人物的拥挤场景数据集上优于现有的架构。本文的技术贡献是一组针对对象集的新的损失函数，同时结合了定位和检测的要素。 另一技术贡献是展示了可以成功地利用LSTM单元链来将图像内容解码为可变长度的相干实值输出。我们预见这种技术在其他结构化计算机视觉预测任务中很有价值，例如多人物追踪和多人物关节姿态估计。

## 1.1. Related work

在存在遮挡的情况下检测多个对象已经是计算机视觉中的臭名昭著的问题。早期的工作采用了局部特征和霍夫投票的码本\[13,2\]，但仍然需要复杂的调整和多级流水线。重要的是，这些模型使用基于局部特征的弱表示，其优于现代深度表示。

为了克服在紧邻区域预测多个对象的困难，已经进行了若干尝试来联合预测相关对象\[5,23,15\]。 我们的工作是更一般的，因为我们没有明确定义这些组，而是让模型学习找到隔断实例所需的任何特征。

目前，表现最佳的对象检测器要么通过以滑动窗口方式密集地扫描图像\[17,6,27,16\]或通过使用诸如\[24,21\]的建议机制来操作，并且利用CNN来对一组指定的建议进行分类\[6\]。两种方法都产生描述包含对象的图像区域的边界框，然后通过合并严重重叠的实例来修剪网络输出。这对于具有少量不重叠对象实例的图像工作良好，但是在存在强闭塞的情况下通常会失效。

例如，Faster R-CNN \[16\]学习类独立的建议，随后用CNN分类。 像Faster R-CNN那样，我们从图像中提出了一组边界框，但是不同的是这些预测直接对应于对象实例，并且不需要后处理。Faster R-CNN的输出必然是稀疏的，而我们的系统能够产生任意紧邻对象的预测。

我们的方法与OverFeat模型\[17\]有关。我们依靠回归模块从CNN编码生成框。然而，在我们的例子中，不同的框是作为集成过程的一部分生成的，而不是像OverFeat中那样独立。因此，每个输出框直接对应于图像中检测到的一个对象，并且我们不需要合并或非极大值抑制等后处理。我们的方法的另一个重要优点是它会输出对应于每个被端到端训练的输出框的置信度。在OverFeat的情况下，端到端训练的置信预测是不可用的，因为它的输出是启发式合并过程的结果。

我们的工作与\[25\]有关，因为我们模型中的训练目标联合考虑了对多个对象实例的检测。主要区别在于，虽然\[25\]中的模型被训练以优化非极大值抑制（NMS）后处理精度，但它在测试时仍然是执行标准检测和NMS，因此很容易与其他模型遇到相同的困难（例如，抑制对彼此接近的两个对象实例的检测）。相反，我们的模型在测试时是联合生成输出边界框，使得它能够正确地检测甚至强烈遮挡的对象。

我们的工作使用来自最近神经网络模型的工具来预测序列\[11，19\]。如在\[19\]中，我们依靠LSTM来预测可变长度输出。与语言生成不同，检测要求系统在2D输出空间上生成，其缺少自然的线性排序。 MultiBox通过引入一个损失函数来解决这个难题，它允许无序的预测在训练期间进行排列以匹配真值实例\[21\]。Faster R-CNN通过将对象划分为9个具有3个尺度和3个纵横比的类别来解决这个问题，允许网络直接产生多个重叠对象，只要它们具有不同的尺寸\[16\]。

我们基于这些贡献，利用我们的循环解码器的能力来按顺序进行联合预测。除了计算预测与真值的最佳匹配之外，我们的损失函数还鼓励模型按照置信度下降的顺序进行预测。在结构化语音识别和自然语言处理中已经提出了适当的损失函数\[7\]。这里我们提出这样一种针对物体检测的损失函数。

图2：我们的系统首先将图像编码为高维特征块。然后LSTM充当控制器，将该信息解码为一组检测结果。

# 2\. Model

## 2.1 Overview

诸如\[12,20\]的深度卷积架构构成了对各种任务都有效的图像表示。这些架构已被用于检测，尽管主要是通过将它们适配到分类或回归框架中。深度表示具有足够的能力来联合编码多个实例的外观，但是必须用用于多实例预测的组件来增强它们以实现这种潜力。在本文中，我们考虑循环神经网络（RNN），特别是LSTM单元\[8\]作为这样的组件的候选者。使深度CNN与基于RNN的解码器相结合变得有吸引力的关键特性是（1）直接接入强大的深卷积表示的能力和（2）产生可变长度的相干预测的能力。这些属性已成功地在\[11\]中用于生成图像说明，在\[19\]中用于机器翻译。生成相干集合的能力在我们这种情况下尤其重要，因为我们的系统需要记住先前生成的预测并避免对同一目标的多个预测。

我们构造一个模型，首先通过卷积架构（例如\[20\]）将图像编码为高维描述符，然后将该表示解码为一组边界框。作为预测可变长度输出的核心机制，我们建立与一个LSTM单元的循环网络。我们的模型的概述如图2所示。我们在整个图像的跨越区域将其转换成具有1024个维度特征的网格描述符。 这1024维向量汇总了区域的内容并携带了关于对象位置的丰富信息。LSTM从该信息源获取并且在区域的解码中充当控制器。在每一步，LSTM输出新的边界框和对应的置信度，即在该位置处将发现先前未检测到的人。这些边界框将按照置信度降序生成。当LSTM在具有高于预定阈值的置信度的区域中不能再找到另一个框时，就会产生停止符号。这时输出序列将被收集并呈现为该区域中所有对象实例的最终描述。

我们的方法中主要计算流水线仅涉及前馈处理，这使得其能够被快速实现。在现代GPU上，该方法在640×480图像上以每秒6帧的速度运行。

## 2.2 Loss function

第2.1节中介绍的架构预测一组候选边界框以及与每个框相对应的置信度得分。假设是按顺序生成的，并且随后的预测通过LSTM的存储器状态取决于先前的预测。在每次重复时，LSTM输出一个对象边界框`b = {b_pos，b_c}`，其中`b_pos =（b_x，b_y，b_w，b_h）`∈R^4 是边界框的相对位置，宽度和高度，`b_c`∈\[ 0,1\]是置信度的真值。低于预定阈值（例如0.5）的置信度值在测试时将被解释为停止符号。较高的边界框置信度b_c应该指示该边界框更可能对应于真阳性。我们将相应的标准真值边界框集合表示为`G = {b^i | i = 1，...，M}`，并且由模型生成的候选边界框集合为`C = {b^j | j = 1，...，N}`。接下来，我们引入适合于将学习过程引导到期望输出的损失函数。

考虑图3中的例子，它示意性地显示出了具有四个生成假设的检测器，每个假设由其预测步骤编号，其被表示为秩。注意典型的检测错误，如假阳性（假设3），不精确的定位（假设1）和同一真值实例的多重预测（假设1和2）。不同的错误需要不同种类的反馈。在假设1的情况下，边框位置必须被微调。相反，假设3是假阳性，模型应当通过设置低置信度得分来丢弃这个预测。假设2是对已经由假设1预测过的目标的第二次预测，也应该被丢弃。为了捕捉这些关系，我们引入一个匹配算法，为每个标准真值分配唯一的候选假设。该算法返回单射函数`f：G→C`，即 f(i) 是分配给标准真值假设i的候选假设的索引。

给定f，我们在集合G和C上定义损失函数：

其中`l_pos`：

是标准真值位置和候选假设之间的位移，`l_c`是候选框置信度的交叉熵损失，它将与标准真值进行匹配。该交叉熵损失的标签由`y_j`提供，它由匹配函数定义得到：

其中`α`是置信度误差和定位误差之间的折衷项。我们交叉验证设置α = 0.03。注意，对于固定匹配，我们可以通过反向传播这个损失函数的梯度来更新网络。

作为一个原始基线，我们考虑一个基于标准真值边界框的固定顺序的简单匹配策略。我们通过图像位置从上到下和从左到右排序标准真值框。该固定顺序匹配序列化地将候选者分配给排好序的标准真值。我们将这个匹配函数称为“固定顺序”匹配，将其表示为`f_fix`，与其对应的损失函数表示为`L_fix`。

**匈牙利损失**：　固定顺序匹配的限制是当解码过程产生假阳性或假阴性时，它可能不正确地将候选假设分配给标准真值实例。对于`f_fix`选择的任何特定顺序，此问题仍然存在。因此，我们研究考虑C和G中元素之间所有可能的一对一分配的损失函数。

回想一下，我们的模型的原则性目标之一是输出对多个对象的连贯的预测序列。我们将生成过程的停止标准定义为当预测分数低于指定阈值时产生。对于这样的分数阈值来说，要使其有意义，我们必须鼓励模型在序列的早期生成正确的假设，并避免在高置信度之前产生低置信度预测。因此，当两个假设都有效地和同一真值实例重叠时（例如，图3中的假设1和2），我们优选匹配在预测序列中较早出现的假设。

图3：标准真值实例（黑色）与被接受（绿色）和被拒绝（红色）候选匹配的图示。 匹配应该同时遵守优先级（1对2）和定位性（4对3）。

为了形式化这个概念，我们引入以下假设与标准真值的比较函数：

函数**Δ：G×C→N×N×R** 返回一个元组，其中`d_ij`是边界框位置之间的L1距离，`r_j`是LSTM输出的预测序列中的`b_j`的秩或索引，`o_ij`∈{ 0，1}是假设与标准真值实例不充分重叠的惩罚变量。这里，重叠标准要求候选者的中心要位于标准真值边界框的范围内。`o_ij`变量明确区分定位和检测错误。我们定义了一个由Δ产生的元组的词典顺序。也就是说，当评估两个假设中的哪一个将被分配给标准真值时，重叠是最重要的，随后是秩，然后再是细粒度定位。

给定方程2中的比较函数Δ的定义，我们通过匈牙利算法在多项式时间内找到C和G之间的最小成本二分匹配。注意，匈牙利算法适用于具有明确定义的加法和成对比较运算的带边权的任何图。为此，我们定义`（+）`作为元素相加，`（<）`作为词典比较。对于图3中的例子，正确匹配假设1和4将花费（0, 5, 0.4），而匹配1和3将花费（1, 4, 2.3），匹配2和4将花费（0, 6, 0.2）。注意，用于检测重叠的第一项是如何适当地处理那些尽管具有低秩，但离标准真值差太远而不足以成为敏感匹配的假设的情况（如图3中的假设3的情况）。我们将这种匹配的相应损失称为匈牙利损失，并表示为`L_hung`。

我们还考虑`L_hung`的简化版本，其中只有来自C的排名前k = | G | 的预测被考虑用于匹配。 注意，这等效于去除或置零方程2中的成对匹配项`o_ij`。 我们将此损失表示为`L_firstk`。 在第4节，我们在实验上比较了`L_fix`，`L_firstk`和`L_hung`，结果显示`L_hung`效果最好。

**损失函数分析**　我们的网络几乎在所有地方都是可微的（DAE），因为它是DAE函数的组合。 在匹配是局部恒定的邻域中，`L_hung`也是DAE。此外，该匹配在最佳匹配成本为其中任何其他匹配并且所有重叠项严格控制的点的邻域中将是恒定的。 在实践中，这将发生在每次训练的迭代，所以我们有信心使用梯度下降。

# 3\. Implementation details

我们构建了我们的模型，将图像编码成1024维GoogLeNet高层特征的15x20大小的网格。网格中的每个单元具有大小为139×139的感受野，并且被训练以产生与中心64×64区域相交的所有边界框的集合。选择64x64大小，足够大以捕获具有挑战性的局部遮挡相互作用。也可以使用更大的区域，但是在我们这个场景上几乎不能提供额外的帮助，其中很少的遮挡相互作用能够跨越该尺度。 300个不同的LSTM控制器并行运行着，每个对应着网格中的1x1x1024单元。

我们的LSTM单元有250个存储器状态，没有偏置项，没有非线性输出。在每一步，我们将GoogLeNet特性与前一个LSTM单元的输出连接，并将结果馈送到下一个LSTM单元。我们已经通过仅将图像馈送到第一LSTM单元中产生可比较的结果，指出图像的多个呈现可能不是必要的。通过并行地产生完整480×640图像的每个区域，给出了解码过程的有效批处理。

我们的模型必须通过LSTM解码器学习在边界框位置上进行回归。 在训练期间，解码器输出边界框的过完备集合，每个边界框具有对应的置信度。为了简单和批处理效率，过完备集的基数是固定的，而不考虑标准真值框的数量。 这样可以训练LSTM对那些和标准真值接近的框输出高置信度分数和正确定位，而在其他地方输出低置信度分数。因为在匹配期间优先考虑前面的输出，所以模型学习首先输出高置信度，容易的边界框。 在我们的数据集中，很少有区域有超过4个实例，我们将过完备集限制为5个预测。更大数量的预测既不改善性能，也不降低性能。

图4：在被接受的预测（绿色）上拼接新区域的预测（红色）的示例。

**模型训练**：　我们使用Caffe开源深度学习框架\[10\]进行训练和评估。我们模型的解码器部分是一个定制的LSTM实现。我们使用学习率e= 0.2和动量0.5训练。梯度被修剪以在网络上具有0.1的最大2范数。 我们每100,000次迭代将学习率降低为0.8倍。在800,000次迭代达到收敛。我们在LSTM输出上使用概率为0.15的dropout(随机失活)。去掉dropout会减少平均精度（AP）0.01。

每次迭代时，训练在一个图像的所有子区域上进行。跨区域的LSTM解码器的并行性降低了较大批量大小的效率增益。所有权重在区域和LSTM步骤之间绑定。然而，当每一步使用单独的权重连接LSTM输出来预测候选时，我们很惊讶地发现有轻微的性能提高。这些权重在各区域之间保持固定。绑定这些权重，AP会从0.85减少到0.82。

**初始化**：GoogLeNet权重用Imagenet \[3\]上预先训练的权重初始化。微调GoogLeNet的特征以满足解码器的新需求至关重要。没有GoogLeNet微调的训练将使AP减少0.29。

解码器中的所有权重用范围在\[-0.1,0.1\]的均匀分布初始化。典型的LSTM输入激活与我们预训练的GoogLeNet显著不同，激活在\[-80,80\]范围内。为了补偿这种不匹配，我们使用一个缩放层将GoogLeNet激活减少100倍，然后将它们导入LSTM。同样，全连接层输出的初始标准偏差约为0.3，但边界框像素位置和大小在\[-64,64\]中变化。因此，在将回归预测与标准真值比较之前，我们将其乘以因子100。注意，只有当还引入了比例学习速率乘法器时，这些修改才与改变权重初始化等价。

**拼接**：　我们的算法被训练来预测64x64像素区域内的多个边界框。要在测试时将其应用于完整的640x480大小的图像，我们会从图像的15×20网格中的每个区域生成预测，然后使用拼接算法递归地合并网格上连续单元格的预测。

拼接过程如图4所示。在给定的迭代中，令A表示当前所有已接受的边界框预测的集合。我们处理一个新的区域，评估解码器直到产生停止信号并收集新提出的边界框的集合C。这些新的边界框中的一些可能和先前的预测有重合。为了去除对同一对象的多次预测，我们定义了与2.2节中的具有成对损失项Δ'的二分匹配问题：**A×C→N×R**，给定**Δ '（b\_i，b\_j）=（m\_ij，d\_ij）**。这里，`m_ij`表示两个框是否不相交，并且`d_ij`是由框之间的L1距离给出的局部消歧项。如前所述，我们利用匈牙利算法在多项式时间内找到最小成本匹配。我们检查每个匹配对`（b，b^）`，并将不与其匹配项`b`重叠的任何候选项`b^`添加到接受框的集合。这个过程和非极大值抑制之间的重要区别是（1）来自相同区域的框不会相互抑制，（2）每个框最多可以抑制一个其他框。连带地，这允许对实例生成预测，即使它们在图像中明显重叠。

# 4\. Experimental results

**数据集和评估指标**：　我们在两个数据集上评估我们的方法。 我们在一个新的大规模人像数据集上进行主要的开发和评价。我们使用来自公共网络摄像头的视频镜头从繁杂的场景收集图像。我们在下文中将这个数据集称为Brainwash。我们发现在Brainwash中有大量的图像可以使我们专注于方法开发，而不会受到小的训练集大小的限制。然后我们在公开的TUD-Crossing数据集上验证我们的结果\[1\]。我们使用相同的网络架构和相同的超参数值在两个数据集上进行实验。

图5：TUD交叉数据集上的示例检测结果。中间和底部两行分别显示了Faster R-CNN和我们的检测器在工作点的输出具有90％的精度。顶行显示了在应用非极大值抑制之前的Faster R-CNN的输出。

图6：人物检测方法在TUD交叉数据集上的比较。我们包括了使用来自\[1\]的仅包括基本上可见主体的原始真值（a）和使用带有所有人物标记的全部真值（b）获得的结果。

对于Brainwash数据集，我们收集了11917幅图像，其中有91146个被标记的人。我们以100秒的固定间隔从视频镜头中提取图像，以确保图像有大的不同。我们分配1000张图像进行测试和验证，然后留下剩余的所有图像进行训练。训练和测试分片之间不存在时间重叠。生成的训练集包含82906个实例。测试和验证集分别包含4922和3318个人像实例。这些图像是根据在某些例子任务中的表现预先选择的少数几个工人使用亚马逊机械Turk进行标记的。我们标记每个人的头部，以避免边界框位置的歧义。这些标记者需要标记他们能够识别的任何人，即使该人的大部分不可见。收集的图像的示例如图8所示。Brainwash数据集中的图像包括一些挑战，例如小规模的人，部分很强的闭塞，以及服装和外观的很大变化性。

图7：性能评估

我们使用\[4\]中定义的标准协议进行评估。如果一个假设与标准真值边界框的交叉得分大于0.5，则认为该假设是正确的。我们绘制召回率-精确度曲线，并总结图7和图6中每个实验中的平均精度（AP）和相等误差率（EER）。对于Brainwash，我们还分析了每个模型预测一幅图中总人数表现怎样。如在\[14\]中，我们通过计算测试集图像中预测数和真实检测数之间的平均绝对差异来测量计数误差。对于每个模型，在验证集上选择最佳检测阈值，并且在图7中将结果报告为COUNT。

**基准方法**：　我们将我们的方法与Faster-RCNN \[16\]和OverFeat \[17\]模型作比较。由\[9\]提供的OverFeat的原始版本依赖于使用AlexNet \[12\]训练的图像表示。因此，我们将原始版本称为`OverFeat-AlexNet`。由于OverFeat和我们的模型都用Caffe实现，我们可以直接用GoogLeNet架构替换上OverFeat模型。我们将新模型称为`OverFeat-GoogLeNet`模型。在Brainwash数据集上的两个OverFeat变体的比较显示在图7中。我们观察到Overfeat-GoogLeNet的性能明显优于OverFeat-AlexNet。

图8：使用OverFeat-GoogLeNet（顶行）和我们的方法（底行）获得的示例检测结果。结果显示每个模型的输出都有90％的精度。

图9：我们的方法失效的情况。

请注意，我们的模型和OverFeat中使用的图像表示是完全相同的。两者都使用相同的代码，参数，过滤器维度和过滤器数量实现。这给了我们直接比较模型的不同假设生成组件的有趣的可能性。在OverFeat \[17\]的情况下，该分量对应于来自每个后面跟有一轮非极大值抑制操作单元的边界框回归。在我们的模型中，该组件对应于使用产生可变长度输出的LSTM层的解码。我们最好的模型的性能如图7所示，并与OverFeat的两个版本进行比较。

**性能评估**：　我们首先将我们的方法与OverFeat基准方法在Brainwash数据集上进行比较。我们的方法相对于OverFeat有了重大改进，召回率从71％提高到81％。我们还在AP（我们的模型0.78，相对于OverFeat-GoogLeNet的0.67）和人工计数错误（0.76 vs 1.05）上取得了相当大的改善。

图8显示了我们的模型和OverFeat-GoogLeNet获得的检测的几个例子。箭头突出显示我们的模型即使在强闭塞的情况下也可以检测到人。我们模型失败的例子如图9中的红色箭头所示。

我们与文献中关于TUD-Crossing数据集的先前工作相比较。该数据集包括来自拥挤的街道场景的图像，并且已经用于评估Tang等人\[22\]的遮挡特异性检测器。我们在TUD-Brussels数据集\[26\]上训练，因为TUD-Crossing数据集不提供相应的训练集。TUD-Crossing数据集的原始标准真值不包括强烈遮挡的人的标签。为了进一步了解不同方法在强遮挡情况下的性能，我们扩展了标准真值以包括数据集中的所有人。这将标记人数从原始版本的1008增加到完整版本中的1530。我们将我们检测器的结果与Tang等人\[22\]报道的结果，以及由作者Zhang等人\[27\]提供的结果进行比较，Zhang等人的方法代表了当前行人检测技术的最前沿。

使用原始标准真值的结果如图6（a）所示。 有着95％的准确率，我们的方法达到了86％的召回率，相对于Tang等人\[22\]报道的79％ （我们的方法的等误差率为90％，\[22\]为85％）。 注意，\[22\]和类似的方法已被明确地设计为解决多人物的检测并采用手动设计的聚类检测组件，而我们的方法可以直接在输入数据上进行训练。 我们的方法有了改进，并且超过了OverFeat-GoogLeNet基准方法以及Zhang等人最近的方法。 \[27\]。

完整标准真值的结果如图6（b）所示。 注意整体表现的大幅下降，这是由于在完整标准真值中强烈遮挡的人占有较大比例。 我们的方法和\[27\]的方法之间的差异在这种情况下更加显著。 我们的方法达到80％的EER，相比于\[27\]的70％。

**与[Faster R-CNN](https://link.jianshu.com?t=https://arxiv.org/abs/1506.01497)对比**：　我们使用作者提供的实现方法，在Brainwash和TUD-Crossing数据集上训练和评估Faster R-CNN检测器\[16\]。结果显示在图6和图7中。我们观察到，对于Faster R-CNN，非极大值抑制（NMS）的最佳水平对于获得良好的性能是至关重要的。我们比较由参数`τ∈[0,1]`控制的三个级别的NMS。在TUD-Crossing数据集上，我们的方法比所有NMS级别的Faster-RCNN都有所改善。在Brainwash数据集上，它表现出了与最佳设置的Faster-RCNN同等的结果。注意，与TUD-Crossing数据集相比，Brainwash场景不那么拥挤，并且包含更低比率的重叠边界框。参数`τ= 0.75`的Faster R-CNN一直对同一个人产生多个预测，导致比较差的准确率。参数`τ= 0.25`的更严格的NMS减轻了这个问题。在TUD-Crossing数据集上，参数`τ= 0.25`去除了大量导致低召回率的预测框，设置`τ= 0.75`保留对接近的人的检测，但对单个人引入了假阳性。我们在图5中展示了我们的方法和Faster R-CNN之间的定性比较。两种方法在完全可见的人的情况下表现相当，但是我们的方法能够更好地检测部分被遮挡的人。

( 译者注：更过关于CNN物体检测可参考 [\[Detection\] CNN 之 "物体检测" 篇](https://www.jianshu.com/p/067f6a989d31) )

在图7中，我们还包括使用我们的模型扩展的结果，其具有额外的重新缩放层，能够在分类之前将特征转换成可变尺度表示，并且得到性能的进一步改善。关于这个扩展的细节我们参考了\[18\]。

**损失函数的比较**　我们现在评估在2.2节中介绍的损失函数。使用`L_fix`损失函数训练的模型只能达到0.60 的AP。这表明允许LSTM在训练期间输出从易到难的检测，而不是以一些固定的空间排序，对于性能表现是很重要的。为了探究重叠项在我们的损失函数中的重要性，我们评估`L_firstk`损失，其将每个区域中的k个标准真值实例匹配到前k个输出预测。我们观察到`L_firstk`通过在训练期间允许变动LSTM输出，在测试时效果优于`L_fix`。然而，我们发现`L_firstk`努力将置信度附加到特定的框位置。使用`L_firstk`，早期置信度预测通常太高，而晚期预测太低。看起来，代替学习相应框是否正确的概率，模型在第i个循环步骤上学习预测在区域中至少有i个人的置信度。这些置信度不适合于检测阈值，并且强调在匹配函数中包括重叠项`o_ij`的重要性。每个损失函数的准确率-召回率曲线如图7所示。

# 5\. Conclusion

在本文中，我们介绍了一种新的对象检测方法，并演示了其在TUD-Crossing和Brainwash数据集上的性能。我们的系统解决了通过从图像的丰富的中间表示解码可变数量的输出来检测多个部分遮蔽的实例的挑战。为了教我们的模型产生一致的预测集合，我们定义了一个适合于端到端训练我们的系统的损失函数。我们的方法在现代GPU上以每秒15帧的速度运行。 我们预见这种方法可以证明在具有结构化输出的其他预测任务（例如人物追踪和关节姿态估计）中也是有效的。

**Acknowledgements.**　这项工作得到了Max Planck视觉计算和通信中心的支持。作者要感谢NVIDIA公司提供K40 GPU。 作者还要感谢Will Song和Brody Huval的有益的讨论。

# References

> \[1\] M. Andriluka, S. Roth, and B. Schiele. People-tracking-by-detection and people-detection-by-tracking. In _CVPR 2008_.

> \[2\] O. Barinova, V. Lempitsky, and P. Kohli. On detection of multiple object instances using Hough transform. In _CVPR 2010_.

> \[3\] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In _CVPR 2009_.

> \[4\] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object classes challenge: A retrospective. _International Journal of Computer Vision_, 111(1):98–136, January 2015.

> \[5\] A. Farhadi and M.A. Sadeghi. Recognition using visual phrases. In _CVPR 2011_.

> \[6\] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In _CVPR’14_.

> \[7\] A. Graves, S. Fernandez, F. Gomez, and J. Schmidhuber. Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks. In _ICML 2006_.

> \[8\] Sepp Hochreiter and Juergen Schmidhuber. Long short-term memory. In _Neural Computation 9, 1997_.

> \[9\] Brody Huval, Tao Wang, Sameep Tandon, Jeff Kiske, Will Song, Joel Pazhayampallil, Mykhaylo Andriluka, Pranav Rajpurkar, Toki Migimatsu, Royce Cheng-Yue, Fernando Mujica, Adam Coates, and Andrew Y. Ng. An empirical evaluation of deep learning on highway driving. _CoRR, abs/1504.01716, 2015_.

> \[10\] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. _arXiv preprint arXiv:1408.5093, 2014_.

> \[11\] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In _CVPR’15_.

> \[12\] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In _NIPS’12_.

> \[13\] B. Leibe, E. Seemann, and B. Schiele. Pedestrian detection in crowded scenes. In _CVPR 2005_.

> \[14\] Victor Lempitsky and Andrew Zisserman. Learning to count objects in images. In _NIPS’10_.

> \[15\] W. Ouyang and X. Wang. Single-pedestrian detection aided by multi-pedestrian detection. In _CVPR’13_.

> \[16\] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In _Neural Information Processing Systems (NIPS), 2015_.

> \[17\] Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, and Yann LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. In _ICLR’14_.

> \[18\] Russell Stewart and Mykhaylo Andriluka. End-to-end people detection in crowded scenes. arXiv preprint _arXiv:1506.04878, 2015_.

> \[19\] Ilya Sutskever, Oriol Vinyals, and Quoc Le. Sequence to sequence learning with neural networks. In _NIPS*2014_.

> \[20\] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. _CoRR, abs/1409.4842, 2014_.

> \[21\] Christian Szegedy, Scott Reed, Dumitru Erhan, and Dragomir Anguelov. Scalable, high-quality object detection. _CoRR, abs/1412.1441, 2014_.

> \[22\] S. Tang, M. Andriluka, A. Milan, K. Schindler, S. Roth, and B. Schiele. Learning people detectors for tracking in crowded scenes. In _ICCV’13_.

> \[23\] Siyu Tang, Mykhaylo Andriluka, and Bernt Schiele. Detection and tracking of occluded people. In _BMVC 2012_.

> \[24\] J.R.R. Uijlings, K.E.A. van de Sande, T. Gevers, and A.W.M. Smeulders. Selective search for object recognition. _International Journal of Computer Vision, 2013_.

> \[25\] Li Wan, David Eigen, and Rob Fergus. End-to-end integration of a convolutional network, deformable parts model and non-maximum suppression. In _CVPR’15_.

> \[26\] Christian Wojek, Stefan Walk, and Bernt Schiele. Multi-cue onboard pedestrian detection. In _CVPR 2009_.

> \[27\] S. Zhang, R. Benenson, and B. Schiele. Filtered channel features for pedestrian detection. In _CVPR, 2015_.

( 时间仓促，很多地方翻译的有问题，仅供参考，推荐阅读[英文原文](https://link.jianshu.com?t=https://arxiv.org/pdf/1506.04878.pdf) )

* * *

# 附录

不想下载文档的可以大致参考一下英文原文预览图：

1 / 9

2 / 9

3 / 9

4 / 9

5 / 9

6 / 9

7 / 9

8 / 9

9 / 9
